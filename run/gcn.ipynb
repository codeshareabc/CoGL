{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mins/.local/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary config/cache directory at /tmp/104723.tmpdir/matplotlib-i18amlwx because the default path (/home/jovyan/.cache/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "\n",
    "import time\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior() \n",
    "import numpy as np\n",
    "import models.graph_gcn as mg\n",
    "import models.adversarialNets as ma\n",
    "import scipy.sparse\n",
    "from utils import data_process, sparse\n",
    "from utils import configs, metrics, feat2struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_ratio: 120\n",
      "label_distribution: [('0', 20), ('1', 20), ('2', 20), ('3', 20), ('4', 20), ('5', 20)]\n",
      "balance_num: 20\n",
      "(3312, 3703) (3312, 3312) (3312, 6) (120,) (1000,)\n",
      "(array([[   0,    0],\n",
      "       [   0,    1],\n",
      "       [   0,    2],\n",
      "       ...,\n",
      "       [3311, 2536],\n",
      "       [3311, 3073],\n",
      "       [3311, 3589]], dtype=int32), array([1., 1., 1., ..., 1., 1., 1.]), (3312, 3703))\n",
      "WARNING:tensorflow:From /home/mins/.local/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:201: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/mins/.local/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:201: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Epoch: 0001 train_loss= 1.80574 train_acc= 0.14167 test_acc= 0.29700 time= 0.11727\n",
      "Epoch: 0002 train_loss= 1.79971 train_acc= 0.27500 test_acc= 0.38500 time= 0.13963\n",
      "Epoch: 0003 train_loss= 1.78854 train_acc= 0.41667 test_acc= 0.44000 time= 0.16062\n",
      "Epoch: 0004 train_loss= 1.77934 train_acc= 0.55833 test_acc= 0.47200 time= 0.18187\n",
      "Epoch: 0005 train_loss= 1.76761 train_acc= 0.59167 test_acc= 0.48600 time= 0.21004\n",
      "Epoch: 0006 train_loss= 1.75622 train_acc= 0.65833 test_acc= 0.49600 time= 0.24870\n",
      "Epoch: 0007 train_loss= 1.73959 train_acc= 0.68333 test_acc= 0.51500 time= 0.27403\n",
      "Epoch: 0008 train_loss= 1.72692 train_acc= 0.74167 test_acc= 0.52500 time= 0.29728\n",
      "Epoch: 0009 train_loss= 1.70802 train_acc= 0.78333 test_acc= 0.53800 time= 0.32510\n",
      "Epoch: 0010 train_loss= 1.68743 train_acc= 0.76667 test_acc= 0.54200 time= 0.35979\n",
      "Epoch: 0011 train_loss= 1.66748 train_acc= 0.77500 test_acc= 0.55400 time= 0.38247\n",
      "Epoch: 0012 train_loss= 1.65195 train_acc= 0.79167 test_acc= 0.56500 time= 0.40365\n",
      "Epoch: 0013 train_loss= 1.62470 train_acc= 0.85833 test_acc= 0.57100 time= 0.42475\n",
      "Epoch: 0014 train_loss= 1.61170 train_acc= 0.85000 test_acc= 0.58300 time= 0.44651\n",
      "Epoch: 0015 train_loss= 1.58960 train_acc= 0.82500 test_acc= 0.59900 time= 0.46710\n",
      "Epoch: 0016 train_loss= 1.55953 train_acc= 0.84167 test_acc= 0.61200 time= 0.48768\n",
      "Epoch: 0017 train_loss= 1.55518 train_acc= 0.84167 test_acc= 0.61700 time= 0.51243\n",
      "Epoch: 0018 train_loss= 1.53831 train_acc= 0.88333 test_acc= 0.62600 time= 0.53353\n",
      "Epoch: 0019 train_loss= 1.52097 train_acc= 0.84167 test_acc= 0.63700 time= 0.55399\n",
      "Epoch: 0020 train_loss= 1.48940 train_acc= 0.85833 test_acc= 0.65000 time= 0.57368\n",
      "Epoch: 0021 train_loss= 1.46398 train_acc= 0.89167 test_acc= 0.65300 time= 0.59363\n",
      "Epoch: 0022 train_loss= 1.45265 train_acc= 0.85833 test_acc= 0.65600 time= 0.61411\n",
      "Epoch: 0023 train_loss= 1.44081 train_acc= 0.88333 test_acc= 0.65800 time= 0.63415\n",
      "Epoch: 0024 train_loss= 1.43629 train_acc= 0.87500 test_acc= 0.66700 time= 0.65399\n",
      "Epoch: 0025 train_loss= 1.40464 train_acc= 0.85000 test_acc= 0.67200 time= 0.67338\n",
      "Epoch: 0026 train_loss= 1.38372 train_acc= 0.89167 test_acc= 0.67400 time= 0.69443\n",
      "Epoch: 0027 train_loss= 1.36782 train_acc= 0.93333 test_acc= 0.67800 time= 0.71497\n",
      "Epoch: 0028 train_loss= 1.35673 train_acc= 0.89167 test_acc= 0.67700 time= 0.73538\n",
      "Epoch: 0029 train_loss= 1.33994 train_acc= 0.92500 test_acc= 0.67900 time= 0.75545\n",
      "Epoch: 0030 train_loss= 1.32611 train_acc= 0.94167 test_acc= 0.68100 time= 0.77522\n",
      "Epoch: 0031 train_loss= 1.30518 train_acc= 0.93333 test_acc= 0.68200 time= 0.79873\n",
      "Epoch: 0032 train_loss= 1.29730 train_acc= 0.95833 test_acc= 0.68500 time= 0.82594\n",
      "Epoch: 0033 train_loss= 1.31087 train_acc= 0.93333 test_acc= 0.69200 time= 0.85817\n",
      "Epoch: 0034 train_loss= 1.29133 train_acc= 0.95000 test_acc= 0.69500 time= 0.87931\n",
      "Epoch: 0035 train_loss= 1.29335 train_acc= 0.93333 test_acc= 0.69700 time= 0.90033\n",
      "Epoch: 0036 train_loss= 1.25647 train_acc= 0.96667 test_acc= 0.70000 time= 0.92091\n",
      "Epoch: 0037 train_loss= 1.26380 train_acc= 0.95000 test_acc= 0.70200 time= 0.94139\n",
      "Epoch: 0038 train_loss= 1.24995 train_acc= 0.96667 test_acc= 0.70200 time= 0.96166\n",
      "Epoch: 0039 train_loss= 1.24514 train_acc= 0.96667 test_acc= 0.70200 time= 0.98290\n",
      "Epoch: 0040 train_loss= 1.22649 train_acc= 0.96667 test_acc= 0.70000 time= 1.01365\n",
      "Epoch: 0041 train_loss= 1.22608 train_acc= 0.96667 test_acc= 0.70300 time= 1.05412\n",
      "Epoch: 0042 train_loss= 1.21402 train_acc= 0.95833 test_acc= 0.70300 time= 1.09449\n",
      "Epoch: 0043 train_loss= 1.22430 train_acc= 0.97500 test_acc= 0.70300 time= 1.11969\n",
      "Epoch: 0044 train_loss= 1.20363 train_acc= 0.96667 test_acc= 0.70600 time= 1.14032\n",
      "Epoch: 0045 train_loss= 1.20937 train_acc= 0.96667 test_acc= 0.70700 time= 1.16038\n",
      "Epoch: 0046 train_loss= 1.19981 train_acc= 0.96667 test_acc= 0.70600 time= 1.18000\n",
      "Epoch: 0047 train_loss= 1.21465 train_acc= 0.97500 test_acc= 0.70500 time= 1.19982\n",
      "Epoch: 0048 train_loss= 1.20269 train_acc= 0.97500 test_acc= 0.70500 time= 1.22395\n",
      "Epoch: 0049 train_loss= 1.18564 train_acc= 0.97500 test_acc= 0.70400 time= 1.24496\n",
      "Epoch: 0050 train_loss= 1.19256 train_acc= 0.97500 test_acc= 0.70500 time= 1.26527\n",
      "Epoch: 0051 train_loss= 1.18124 train_acc= 0.98333 test_acc= 0.70400 time= 1.28569\n",
      "Epoch: 0052 train_loss= 1.18444 train_acc= 0.96667 test_acc= 0.70500 time= 1.30568\n",
      "Epoch: 0053 train_loss= 1.18460 train_acc= 0.97500 test_acc= 0.70500 time= 1.33148\n",
      "Epoch: 0054 train_loss= 1.17349 train_acc= 0.97500 test_acc= 0.70400 time= 1.36600\n",
      "Epoch: 0055 train_loss= 1.17694 train_acc= 0.97500 test_acc= 0.70400 time= 1.38756\n",
      "Epoch: 0056 train_loss= 1.17489 train_acc= 0.96667 test_acc= 0.70000 time= 1.40762\n",
      "Epoch: 0057 train_loss= 1.18014 train_acc= 0.97500 test_acc= 0.70100 time= 1.42716\n",
      "Epoch: 0058 train_loss= 1.17046 train_acc= 0.98333 test_acc= 0.69800 time= 1.44944\n",
      "Epoch: 0059 train_loss= 1.17207 train_acc= 0.98333 test_acc= 0.69800 time= 1.48647\n",
      "Epoch: 0060 train_loss= 1.15969 train_acc= 0.98333 test_acc= 0.69700 time= 1.52071\n",
      "Epoch: 0061 train_loss= 1.15906 train_acc= 0.98333 test_acc= 0.69400 time= 1.54723\n",
      "Epoch: 0062 train_loss= 1.14570 train_acc= 0.99167 test_acc= 0.69200 time= 1.57480\n",
      "Epoch: 0063 train_loss= 1.16891 train_acc= 0.98333 test_acc= 0.69000 time= 1.62085\n",
      "Epoch: 0064 train_loss= 1.17082 train_acc= 0.96667 test_acc= 0.69000 time= 1.65076\n",
      "Epoch: 0065 train_loss= 1.14651 train_acc= 0.99167 test_acc= 0.68900 time= 1.67800\n",
      "Epoch: 0066 train_loss= 1.15179 train_acc= 0.97500 test_acc= 0.68900 time= 1.69913\n",
      "Epoch: 0067 train_loss= 1.14854 train_acc= 0.99167 test_acc= 0.68900 time= 1.71887\n",
      "Epoch: 0068 train_loss= 1.15728 train_acc= 0.99167 test_acc= 0.68800 time= 1.73968\n",
      "Epoch: 0069 train_loss= 1.15321 train_acc= 0.98333 test_acc= 0.68700 time= 1.76045\n",
      "Epoch: 0070 train_loss= 1.16042 train_acc= 0.99167 test_acc= 0.68500 time= 1.78146\n",
      "Epoch: 0071 train_loss= 1.14753 train_acc= 0.99167 test_acc= 0.68600 time= 1.80251\n",
      "Epoch: 0072 train_loss= 1.16613 train_acc= 0.97500 test_acc= 0.68600 time= 1.82329\n",
      "Epoch: 0073 train_loss= 1.15258 train_acc= 1.00000 test_acc= 0.68500 time= 1.84391\n",
      "Epoch: 0074 train_loss= 1.14028 train_acc= 0.99167 test_acc= 0.68500 time= 1.86421\n",
      "Epoch: 0075 train_loss= 1.14619 train_acc= 0.99167 test_acc= 0.68400 time= 1.89318\n",
      "Epoch: 0076 train_loss= 1.14325 train_acc= 0.99167 test_acc= 0.68500 time= 1.91661\n",
      "Epoch: 0077 train_loss= 1.14330 train_acc= 0.98333 test_acc= 0.68700 time= 1.93725\n",
      "Epoch: 0078 train_loss= 1.13153 train_acc= 0.99167 test_acc= 0.68500 time= 1.95820\n",
      "Epoch: 0079 train_loss= 1.13689 train_acc= 0.99167 test_acc= 0.68600 time= 1.97906\n",
      "Epoch: 0080 train_loss= 1.14338 train_acc= 0.99167 test_acc= 0.68800 time= 2.00349\n",
      "Epoch: 0081 train_loss= 1.14418 train_acc= 0.99167 test_acc= 0.68900 time= 2.02436\n",
      "Epoch: 0082 train_loss= 1.14054 train_acc= 1.00000 test_acc= 0.69000 time= 2.04468\n",
      "Epoch: 0083 train_loss= 1.14676 train_acc= 0.99167 test_acc= 0.69100 time= 2.06541\n",
      "Epoch: 0084 train_loss= 1.13862 train_acc= 0.99167 test_acc= 0.68900 time= 2.08667\n",
      "Epoch: 0085 train_loss= 1.14374 train_acc= 0.98333 test_acc= 0.69200 time= 2.12354\n",
      "Epoch: 0086 train_loss= 1.13131 train_acc= 0.99167 test_acc= 0.69300 time= 2.14774\n",
      "Epoch: 0087 train_loss= 1.13476 train_acc= 0.99167 test_acc= 0.69200 time= 2.17296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0088 train_loss= 1.13261 train_acc= 1.00000 test_acc= 0.69000 time= 2.21283\n",
      "Epoch: 0089 train_loss= 1.13215 train_acc= 0.99167 test_acc= 0.68800 time= 2.24242\n",
      "Epoch: 0090 train_loss= 1.13564 train_acc= 0.99167 test_acc= 0.68500 time= 2.26655\n",
      "Epoch: 0091 train_loss= 1.13741 train_acc= 0.99167 test_acc= 0.68200 time= 2.28798\n",
      "Epoch: 0092 train_loss= 1.12409 train_acc= 0.99167 test_acc= 0.68100 time= 2.30960\n",
      "Epoch: 0093 train_loss= 1.15007 train_acc= 0.98333 test_acc= 0.68200 time= 2.32973\n",
      "Epoch: 0094 train_loss= 1.13440 train_acc= 1.00000 test_acc= 0.67900 time= 2.34924\n",
      "Epoch: 0095 train_loss= 1.12964 train_acc= 0.99167 test_acc= 0.68000 time= 2.37052\n",
      "Epoch: 0096 train_loss= 1.13279 train_acc= 0.99167 test_acc= 0.68000 time= 2.40048\n",
      "Epoch: 0097 train_loss= 1.12720 train_acc= 1.00000 test_acc= 0.68300 time= 2.44010\n",
      "Epoch: 0098 train_loss= 1.13096 train_acc= 0.99167 test_acc= 0.68200 time= 2.47974\n",
      "Epoch: 0099 train_loss= 1.12908 train_acc= 0.99167 test_acc= 0.68300 time= 2.50289\n",
      "Epoch: 0100 train_loss= 1.13015 train_acc= 0.99167 test_acc= 0.68500 time= 2.52447\n",
      "Epoch: 0101 train_loss= 1.13003 train_acc= 0.99167 test_acc= 0.68600 time= 2.54520\n",
      "Epoch: 0102 train_loss= 1.12778 train_acc= 0.99167 test_acc= 0.68700 time= 2.56603\n",
      "Epoch: 0103 train_loss= 1.12448 train_acc= 0.99167 test_acc= 0.68700 time= 2.58701\n",
      "Epoch: 0104 train_loss= 1.12864 train_acc= 0.99167 test_acc= 0.68800 time= 2.61057\n",
      "Epoch: 0105 train_loss= 1.12451 train_acc= 0.99167 test_acc= 0.68700 time= 2.63298\n",
      "Epoch: 0106 train_loss= 1.13506 train_acc= 0.99167 test_acc= 0.68700 time= 2.65385\n",
      "Epoch: 0107 train_loss= 1.13153 train_acc= 0.99167 test_acc= 0.68500 time= 2.67384\n",
      "Epoch: 0108 train_loss= 1.12967 train_acc= 0.98333 test_acc= 0.68400 time= 2.69429\n",
      "Epoch: 0109 train_loss= 1.12900 train_acc= 0.99167 test_acc= 0.68200 time= 2.71743\n",
      "Epoch: 0110 train_loss= 1.11791 train_acc= 1.00000 test_acc= 0.68000 time= 2.75052\n",
      "Epoch: 0111 train_loss= 1.12813 train_acc= 1.00000 test_acc= 0.67900 time= 2.77687\n",
      "Epoch: 0112 train_loss= 1.12841 train_acc= 0.98333 test_acc= 0.67900 time= 2.79841\n",
      "Epoch: 0113 train_loss= 1.12693 train_acc= 0.99167 test_acc= 0.68100 time= 2.81971\n",
      "Epoch: 0114 train_loss= 1.12435 train_acc= 1.00000 test_acc= 0.68100 time= 2.85438\n",
      "Epoch: 0115 train_loss= 1.12163 train_acc= 0.99167 test_acc= 0.68400 time= 2.88943\n",
      "Epoch: 0116 train_loss= 1.12083 train_acc= 1.00000 test_acc= 0.68500 time= 2.91300\n",
      "Epoch: 0117 train_loss= 1.11782 train_acc= 0.99167 test_acc= 0.68600 time= 2.94657\n",
      "Epoch: 0118 train_loss= 1.12732 train_acc= 0.99167 test_acc= 0.68600 time= 2.98426\n",
      "Epoch: 0119 train_loss= 1.13108 train_acc= 0.99167 test_acc= 0.68300 time= 3.01101\n",
      "Epoch: 0120 train_loss= 1.11806 train_acc= 0.99167 test_acc= 0.68100 time= 3.04001\n",
      "Epoch: 0121 train_loss= 1.12248 train_acc= 0.99167 test_acc= 0.67900 time= 3.06180\n",
      "Epoch: 0122 train_loss= 1.12483 train_acc= 0.98333 test_acc= 0.67900 time= 3.08359\n",
      "Epoch: 0123 train_loss= 1.11474 train_acc= 1.00000 test_acc= 0.68000 time= 3.10533\n",
      "Epoch: 0124 train_loss= 1.11712 train_acc= 0.99167 test_acc= 0.67900 time= 3.12596\n",
      "Epoch: 0125 train_loss= 1.11289 train_acc= 0.99167 test_acc= 0.67900 time= 3.14651\n",
      "Epoch: 0126 train_loss= 1.11686 train_acc= 1.00000 test_acc= 0.67700 time= 3.16811\n",
      "Epoch: 0127 train_loss= 1.11578 train_acc= 0.99167 test_acc= 0.68000 time= 3.18836\n",
      "Epoch: 0128 train_loss= 1.11920 train_acc= 0.99167 test_acc= 0.68000 time= 3.20852\n",
      "Epoch: 0129 train_loss= 1.11152 train_acc= 0.99167 test_acc= 0.68100 time= 3.22857\n",
      "Epoch: 0130 train_loss= 1.11744 train_acc= 0.99167 test_acc= 0.68300 time= 3.24803\n",
      "Epoch: 0131 train_loss= 1.12581 train_acc= 0.99167 test_acc= 0.68300 time= 3.26758\n",
      "Epoch: 0132 train_loss= 1.12066 train_acc= 1.00000 test_acc= 0.68300 time= 3.28712\n",
      "Epoch: 0133 train_loss= 1.11209 train_acc= 1.00000 test_acc= 0.68400 time= 3.30656\n",
      "Epoch: 0134 train_loss= 1.11191 train_acc= 0.99167 test_acc= 0.68300 time= 3.32619\n",
      "Epoch: 0135 train_loss= 1.11653 train_acc= 0.99167 test_acc= 0.68300 time= 3.34622\n",
      "Epoch: 0136 train_loss= 1.11684 train_acc= 1.00000 test_acc= 0.68000 time= 3.36570\n",
      "Epoch: 0137 train_loss= 1.11880 train_acc= 0.99167 test_acc= 0.67900 time= 3.38626\n",
      "Epoch: 0138 train_loss= 1.11541 train_acc= 1.00000 test_acc= 0.68000 time= 3.40647\n",
      "Epoch: 0139 train_loss= 1.11076 train_acc= 0.99167 test_acc= 0.68100 time= 3.42653\n",
      "Epoch: 0140 train_loss= 1.11758 train_acc= 0.99167 test_acc= 0.68100 time= 3.44620\n",
      "Epoch: 0141 train_loss= 1.12692 train_acc= 0.99167 test_acc= 0.68200 time= 3.46800\n",
      "Epoch: 0142 train_loss= 1.11110 train_acc= 1.00000 test_acc= 0.68200 time= 3.49332\n",
      "Epoch: 0143 train_loss= 1.11735 train_acc= 0.98333 test_acc= 0.68200 time= 3.51684\n",
      "Epoch: 0144 train_loss= 1.11865 train_acc= 1.00000 test_acc= 0.68100 time= 3.53815\n",
      "Epoch: 0145 train_loss= 1.10909 train_acc= 1.00000 test_acc= 0.68100 time= 3.56097\n",
      "Epoch: 0146 train_loss= 1.11563 train_acc= 0.99167 test_acc= 0.67800 time= 3.59178\n",
      "Epoch: 0147 train_loss= 1.11057 train_acc= 0.99167 test_acc= 0.67900 time= 3.61624\n",
      "Epoch: 0148 train_loss= 1.11588 train_acc= 1.00000 test_acc= 0.68100 time= 3.63830\n",
      "Epoch: 0149 train_loss= 1.10801 train_acc= 1.00000 test_acc= 0.68100 time= 3.66024\n",
      "Epoch: 0150 train_loss= 1.10971 train_acc= 0.99167 test_acc= 0.68400 time= 3.68120\n",
      "Epoch: 0151 train_loss= 1.11178 train_acc= 0.99167 test_acc= 0.68500 time= 3.70236\n",
      "Epoch: 0152 train_loss= 1.10724 train_acc= 1.00000 test_acc= 0.68600 time= 3.72344\n",
      "Epoch: 0153 train_loss= 1.10692 train_acc= 1.00000 test_acc= 0.68700 time= 3.74379\n",
      "Epoch: 0154 train_loss= 1.11622 train_acc= 0.99167 test_acc= 0.68700 time= 3.76492\n",
      "Epoch: 0155 train_loss= 1.10647 train_acc= 1.00000 test_acc= 0.68600 time= 3.78551\n",
      "Epoch: 0156 train_loss= 1.11207 train_acc= 1.00000 test_acc= 0.68700 time= 3.80539\n",
      "Epoch: 0157 train_loss= 1.10892 train_acc= 0.99167 test_acc= 0.68700 time= 3.82511\n",
      "Epoch: 0158 train_loss= 1.11193 train_acc= 0.99167 test_acc= 0.68700 time= 3.84567\n",
      "Epoch: 0159 train_loss= 1.10880 train_acc= 1.00000 test_acc= 0.68400 time= 3.86555\n",
      "Epoch: 0160 train_loss= 1.11377 train_acc= 0.99167 test_acc= 0.68500 time= 3.88560\n",
      "Epoch: 0161 train_loss= 1.10937 train_acc= 1.00000 test_acc= 0.68400 time= 3.90517\n",
      "Epoch: 0162 train_loss= 1.11341 train_acc= 1.00000 test_acc= 0.68400 time= 3.92508\n",
      "Epoch: 0163 train_loss= 1.11013 train_acc= 1.00000 test_acc= 0.68100 time= 3.94533\n",
      "Epoch: 0164 train_loss= 1.11173 train_acc= 0.99167 test_acc= 0.68000 time= 3.96488\n",
      "Epoch: 0165 train_loss= 1.11177 train_acc= 0.99167 test_acc= 0.68000 time= 3.98444\n",
      "Epoch: 0166 train_loss= 1.10992 train_acc= 0.99167 test_acc= 0.68100 time= 4.00448\n",
      "Epoch: 0167 train_loss= 1.11212 train_acc= 1.00000 test_acc= 0.68100 time= 4.02447\n",
      "Epoch: 0168 train_loss= 1.12010 train_acc= 0.99167 test_acc= 0.68300 time= 4.04470\n",
      "Epoch: 0169 train_loss= 1.10906 train_acc= 1.00000 test_acc= 0.68300 time= 4.08141\n",
      "Epoch: 0170 train_loss= 1.11120 train_acc= 0.99167 test_acc= 0.68300 time= 4.10886\n",
      "Epoch: 0171 train_loss= 1.10661 train_acc= 0.99167 test_acc= 0.68500 time= 4.13141\n",
      "Epoch: 0172 train_loss= 1.11358 train_acc= 1.00000 test_acc= 0.68400 time= 4.15199\n",
      "Epoch: 0173 train_loss= 1.10426 train_acc= 1.00000 test_acc= 0.68600 time= 4.17283\n",
      "Epoch: 0174 train_loss= 1.10566 train_acc= 1.00000 test_acc= 0.68500 time= 4.19461\n",
      "Epoch: 0175 train_loss= 1.10905 train_acc= 1.00000 test_acc= 0.68600 time= 4.21642\n",
      "Epoch: 0176 train_loss= 1.10590 train_acc= 1.00000 test_acc= 0.68600 time= 4.23678\n",
      "Epoch: 0177 train_loss= 1.10201 train_acc= 0.99167 test_acc= 0.68500 time= 4.26835\n",
      "Epoch: 0178 train_loss= 1.11226 train_acc= 0.99167 test_acc= 0.68700 time= 4.30657\n",
      "Epoch: 0179 train_loss= 1.10608 train_acc= 1.00000 test_acc= 0.68800 time= 4.34773\n",
      "Epoch: 0180 train_loss= 1.10639 train_acc= 0.99167 test_acc= 0.68700 time= 4.36991\n",
      "Epoch: 0181 train_loss= 1.10388 train_acc= 1.00000 test_acc= 0.68800 time= 4.39145\n",
      "Epoch: 0182 train_loss= 1.09974 train_acc= 1.00000 test_acc= 0.68700 time= 4.41267\n",
      "Epoch: 0183 train_loss= 1.10387 train_acc= 1.00000 test_acc= 0.68800 time= 4.43415\n",
      "Epoch: 0184 train_loss= 1.10775 train_acc= 0.99167 test_acc= 0.68900 time= 4.45554\n",
      "Epoch: 0185 train_loss= 1.10717 train_acc= 0.99167 test_acc= 0.68900 time= 4.48209\n",
      "Epoch: 0186 train_loss= 1.11448 train_acc= 0.99167 test_acc= 0.68900 time= 4.50330\n",
      "Epoch: 0187 train_loss= 1.10268 train_acc= 0.99167 test_acc= 0.68900 time= 4.52359\n",
      "Epoch: 0188 train_loss= 1.10173 train_acc= 1.00000 test_acc= 0.68900 time= 4.54385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0189 train_loss= 1.10917 train_acc= 1.00000 test_acc= 0.68900 time= 4.56419\n",
      "Epoch: 0190 train_loss= 1.10636 train_acc= 0.99167 test_acc= 0.68800 time= 4.59017\n",
      "Epoch: 0191 train_loss= 1.10243 train_acc= 0.99167 test_acc= 0.68800 time= 4.61631\n",
      "Epoch: 0192 train_loss= 1.10130 train_acc= 1.00000 test_acc= 0.68500 time= 4.63813\n",
      "Epoch: 0193 train_loss= 1.10683 train_acc= 1.00000 test_acc= 0.68200 time= 4.65835\n",
      "Epoch: 0194 train_loss= 1.10584 train_acc= 0.99167 test_acc= 0.68300 time= 4.67922\n",
      "Epoch: 0195 train_loss= 1.10575 train_acc= 0.98333 test_acc= 0.68300 time= 4.70576\n",
      "Epoch: 0196 train_loss= 1.10504 train_acc= 0.99167 test_acc= 0.68000 time= 4.74399\n",
      "Epoch: 0197 train_loss= 1.10338 train_acc= 1.00000 test_acc= 0.68100 time= 4.78296\n",
      "Epoch: 0198 train_loss= 1.10674 train_acc= 0.99167 test_acc= 0.68300 time= 4.81164\n",
      "Epoch: 0199 train_loss= 1.10582 train_acc= 0.99167 test_acc= 0.68200 time= 4.85117\n",
      "Epoch: 0200 train_loss= 1.10812 train_acc= 0.99167 test_acc= 0.68600 time= 4.88085\n",
      "Epoch: 0201 train_loss= 1.10430 train_acc= 0.99167 test_acc= 0.68600 time= 4.91579\n",
      "Epoch: 0202 train_loss= 1.10638 train_acc= 0.99167 test_acc= 0.68600 time= 4.95552\n",
      "Epoch: 0203 train_loss= 1.10514 train_acc= 0.99167 test_acc= 0.68700 time= 4.97898\n",
      "Epoch: 0204 train_loss= 1.09600 train_acc= 1.00000 test_acc= 0.69100 time= 4.99954\n",
      "Epoch: 0205 train_loss= 1.10205 train_acc= 0.99167 test_acc= 0.69200 time= 5.01985\n",
      "Epoch: 0206 train_loss= 1.10420 train_acc= 0.99167 test_acc= 0.69000 time= 5.04009\n",
      "Epoch: 0207 train_loss= 1.10999 train_acc= 0.99167 test_acc= 0.69000 time= 5.06080\n",
      "Epoch: 0208 train_loss= 1.09977 train_acc= 1.00000 test_acc= 0.69100 time= 5.08105\n",
      "Epoch: 0209 train_loss= 1.10396 train_acc= 0.99167 test_acc= 0.69100 time= 5.10144\n",
      "Epoch: 0210 train_loss= 1.10800 train_acc= 0.99167 test_acc= 0.68900 time= 5.12217\n",
      "Epoch: 0211 train_loss= 1.10255 train_acc= 0.99167 test_acc= 0.68900 time= 5.14273\n",
      "Epoch: 0212 train_loss= 1.09789 train_acc= 0.99167 test_acc= 0.69000 time= 5.16305\n",
      "Epoch: 0213 train_loss= 1.10765 train_acc= 0.99167 test_acc= 0.68800 time= 5.18320\n",
      "Epoch: 0214 train_loss= 1.09940 train_acc= 0.99167 test_acc= 0.68700 time= 5.20392\n",
      "Epoch: 0215 train_loss= 1.09965 train_acc= 1.00000 test_acc= 0.68500 time= 5.22433\n",
      "Epoch: 0216 train_loss= 1.09775 train_acc= 1.00000 test_acc= 0.68600 time= 5.24567\n",
      "Epoch: 0217 train_loss= 1.09590 train_acc= 1.00000 test_acc= 0.68700 time= 5.26590\n",
      "Epoch: 0218 train_loss= 1.09847 train_acc= 1.00000 test_acc= 0.68700 time= 5.28612\n",
      "Epoch: 0219 train_loss= 1.10868 train_acc= 0.99167 test_acc= 0.68600 time= 5.30678\n",
      "Epoch: 0220 train_loss= 1.10788 train_acc= 0.99167 test_acc= 0.68300 time= 5.32819\n",
      "Epoch: 0221 train_loss= 1.09447 train_acc= 1.00000 test_acc= 0.68300 time= 5.34829\n",
      "Epoch: 0222 train_loss= 1.10270 train_acc= 1.00000 test_acc= 0.68400 time= 5.36852\n",
      "Epoch: 0223 train_loss= 1.09709 train_acc= 1.00000 test_acc= 0.68300 time= 5.39363\n",
      "Epoch: 0224 train_loss= 1.11182 train_acc= 0.99167 test_acc= 0.68300 time= 5.42617\n",
      "Epoch: 0225 train_loss= 1.10137 train_acc= 0.99167 test_acc= 0.68500 time= 5.45007\n",
      "Epoch: 0226 train_loss= 1.10138 train_acc= 0.99167 test_acc= 0.68500 time= 5.47138\n",
      "Epoch: 0227 train_loss= 1.10069 train_acc= 1.00000 test_acc= 0.68500 time= 5.49463\n",
      "Epoch: 0228 train_loss= 1.10187 train_acc= 0.99167 test_acc= 0.68600 time= 5.52002\n",
      "Epoch: 0229 train_loss= 1.09714 train_acc= 1.00000 test_acc= 0.68700 time= 5.54731\n",
      "Epoch: 0230 train_loss= 1.09980 train_acc= 1.00000 test_acc= 0.68700 time= 5.56905\n",
      "Epoch: 0231 train_loss= 1.10271 train_acc= 1.00000 test_acc= 0.68500 time= 5.58978\n",
      "Epoch: 0232 train_loss= 1.10350 train_acc= 0.99167 test_acc= 0.68500 time= 5.61096\n",
      "Epoch: 0233 train_loss= 1.10329 train_acc= 0.99167 test_acc= 0.68600 time= 5.63053\n",
      "Epoch: 0234 train_loss= 1.10109 train_acc= 1.00000 test_acc= 0.68500 time= 5.65040\n",
      "Epoch: 0235 train_loss= 1.09525 train_acc= 1.00000 test_acc= 0.68600 time= 5.67008\n",
      "Epoch: 0236 train_loss= 1.09964 train_acc= 0.99167 test_acc= 0.68600 time= 5.69205\n",
      "Epoch: 0237 train_loss= 1.10379 train_acc= 0.98333 test_acc= 0.68600 time= 5.71844\n",
      "Epoch: 0238 train_loss= 1.10143 train_acc= 1.00000 test_acc= 0.68600 time= 5.77332\n",
      "Epoch: 0239 train_loss= 1.10518 train_acc= 0.99167 test_acc= 0.68600 time= 5.79462\n",
      "Epoch: 0240 train_loss= 1.09833 train_acc= 1.00000 test_acc= 0.68600 time= 5.81480\n",
      "Epoch: 0241 train_loss= 1.10149 train_acc= 1.00000 test_acc= 0.68600 time= 5.83488\n",
      "Epoch: 0242 train_loss= 1.10145 train_acc= 0.99167 test_acc= 0.68600 time= 5.85443\n",
      "Epoch: 0243 train_loss= 1.11092 train_acc= 0.98333 test_acc= 0.68700 time= 5.87414\n",
      "Epoch: 0244 train_loss= 1.09742 train_acc= 0.99167 test_acc= 0.68800 time= 5.89411\n",
      "Epoch: 0245 train_loss= 1.10230 train_acc= 0.99167 test_acc= 0.69000 time= 5.91487\n",
      "Epoch: 0246 train_loss= 1.10199 train_acc= 0.99167 test_acc= 0.69000 time= 5.93533\n",
      "Epoch: 0247 train_loss= 1.09648 train_acc= 1.00000 test_acc= 0.68900 time= 5.95545\n",
      "Epoch: 0248 train_loss= 1.10452 train_acc= 0.99167 test_acc= 0.68900 time= 5.97523\n",
      "Epoch: 0249 train_loss= 1.09791 train_acc= 1.00000 test_acc= 0.69100 time= 5.99605\n",
      "Epoch: 0250 train_loss= 1.10133 train_acc= 0.99167 test_acc= 0.69000 time= 6.02227\n",
      "Epoch: 0251 train_loss= 1.10306 train_acc= 0.99167 test_acc= 0.68900 time= 6.04471\n",
      "Epoch: 0252 train_loss= 1.09758 train_acc= 0.99167 test_acc= 0.68900 time= 6.06558\n",
      "Epoch: 0253 train_loss= 1.09935 train_acc= 0.99167 test_acc= 0.68900 time= 6.08556\n",
      "Epoch: 0254 train_loss= 1.09812 train_acc= 0.99167 test_acc= 0.69000 time= 6.10513\n",
      "Epoch: 0255 train_loss= 1.09861 train_acc= 0.99167 test_acc= 0.68900 time= 6.13561\n",
      "Epoch: 0256 train_loss= 1.09907 train_acc= 0.99167 test_acc= 0.68700 time= 6.17534\n",
      "Epoch: 0257 train_loss= 1.09395 train_acc= 1.00000 test_acc= 0.68800 time= 6.19843\n",
      "Epoch: 0258 train_loss= 1.09346 train_acc= 1.00000 test_acc= 0.68800 time= 6.24298\n",
      "Epoch: 0259 train_loss= 1.09898 train_acc= 1.00000 test_acc= 0.69100 time= 6.28696\n",
      "Epoch: 0260 train_loss= 1.09748 train_acc= 1.00000 test_acc= 0.69100 time= 6.31105\n",
      "Epoch: 0261 train_loss= 1.10024 train_acc= 0.99167 test_acc= 0.69200 time= 6.34517\n",
      "Epoch: 0262 train_loss= 1.09564 train_acc= 1.00000 test_acc= 0.69200 time= 6.36550\n",
      "Epoch: 0263 train_loss= 1.09319 train_acc= 0.99167 test_acc= 0.69400 time= 6.38644\n",
      "Epoch: 0264 train_loss= 1.09622 train_acc= 0.99167 test_acc= 0.69300 time= 6.40791\n",
      "Epoch: 0265 train_loss= 1.09492 train_acc= 1.00000 test_acc= 0.69200 time= 6.42912\n",
      "Epoch: 0266 train_loss= 1.09282 train_acc= 0.99167 test_acc= 0.69200 time= 6.45011\n",
      "Epoch: 0267 train_loss= 1.10169 train_acc= 0.99167 test_acc= 0.69300 time= 6.47051\n",
      "Epoch: 0268 train_loss= 1.09962 train_acc= 0.99167 test_acc= 0.69500 time= 6.49075\n",
      "Epoch: 0269 train_loss= 1.09398 train_acc= 1.00000 test_acc= 0.69500 time= 6.51070\n",
      "Epoch: 0270 train_loss= 1.09915 train_acc= 0.99167 test_acc= 0.69500 time= 6.53103\n",
      "Epoch: 0271 train_loss= 1.09666 train_acc= 0.99167 test_acc= 0.69400 time= 6.55093\n",
      "Epoch: 0272 train_loss= 1.10257 train_acc= 0.99167 test_acc= 0.69400 time= 6.57113\n",
      "Epoch: 0273 train_loss= 1.09977 train_acc= 0.99167 test_acc= 0.69300 time= 6.59100\n",
      "Epoch: 0274 train_loss= 1.09737 train_acc= 0.99167 test_acc= 0.69300 time= 6.61128\n",
      "Epoch: 0275 train_loss= 1.09341 train_acc= 0.99167 test_acc= 0.69100 time= 6.63118\n",
      "Epoch: 0276 train_loss= 1.09734 train_acc= 0.99167 test_acc= 0.69100 time= 6.65063\n",
      "Epoch: 0277 train_loss= 1.09457 train_acc= 1.00000 test_acc= 0.69100 time= 6.67067\n",
      "Epoch: 0278 train_loss= 1.09792 train_acc= 1.00000 test_acc= 0.69100 time= 6.69058\n",
      "Epoch: 0279 train_loss= 1.09928 train_acc= 0.99167 test_acc= 0.69100 time= 6.71117\n",
      "Epoch: 0280 train_loss= 1.09165 train_acc= 1.00000 test_acc= 0.69000 time= 6.73158\n",
      "Epoch: 0281 train_loss= 1.09845 train_acc= 0.99167 test_acc= 0.69100 time= 6.75145\n",
      "Epoch: 0282 train_loss= 1.10486 train_acc= 1.00000 test_acc= 0.69200 time= 6.77166\n",
      "Epoch: 0283 train_loss= 1.09423 train_acc= 1.00000 test_acc= 0.69500 time= 6.79128\n",
      "Epoch: 0284 train_loss= 1.08649 train_acc= 1.00000 test_acc= 0.69500 time= 6.81565\n",
      "Epoch: 0285 train_loss= 1.09703 train_acc= 1.00000 test_acc= 0.69300 time= 6.83817\n",
      "Epoch: 0286 train_loss= 1.09275 train_acc= 1.00000 test_acc= 0.69000 time= 6.86124\n",
      "Epoch: 0287 train_loss= 1.08871 train_acc= 1.00000 test_acc= 0.69000 time= 6.88320\n",
      "Epoch: 0288 train_loss= 1.09303 train_acc= 1.00000 test_acc= 0.69000 time= 6.90870\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0289 train_loss= 1.09061 train_acc= 1.00000 test_acc= 0.69200 time= 6.93489\n",
      "Epoch: 0290 train_loss= 1.08968 train_acc= 1.00000 test_acc= 0.69200 time= 6.96305\n",
      "Epoch: 0291 train_loss= 1.09107 train_acc= 0.99167 test_acc= 0.69100 time= 6.98440\n",
      "Epoch: 0292 train_loss= 1.09069 train_acc= 0.99167 test_acc= 0.69100 time= 7.00534\n",
      "Epoch: 0293 train_loss= 1.09183 train_acc= 0.99167 test_acc= 0.69100 time= 7.02589\n",
      "Epoch: 0294 train_loss= 1.09858 train_acc= 1.00000 test_acc= 0.69000 time= 7.04826\n",
      "Epoch: 0295 train_loss= 1.09614 train_acc= 0.99167 test_acc= 0.68900 time= 7.07013\n",
      "Epoch: 0296 train_loss= 1.09332 train_acc= 1.00000 test_acc= 0.69000 time= 7.09195\n",
      "Epoch: 0297 train_loss= 1.10074 train_acc= 0.98333 test_acc= 0.69000 time= 7.11347\n",
      "Epoch: 0298 train_loss= 1.08793 train_acc= 1.00000 test_acc= 0.68900 time= 7.13384\n",
      "Epoch: 0299 train_loss= 1.09194 train_acc= 1.00000 test_acc= 0.68900 time= 7.15352\n",
      "Epoch: 0300 train_loss= 1.08932 train_acc= 0.99167 test_acc= 0.68900 time= 7.17386\n",
      "Epoch: 0301 train_loss= 1.09839 train_acc= 0.99167 test_acc= 0.68900 time= 7.19349\n",
      "Epoch: 0302 train_loss= 1.09357 train_acc= 1.00000 test_acc= 0.68800 time= 7.21368\n",
      "Epoch: 0303 train_loss= 1.09024 train_acc= 1.00000 test_acc= 0.68900 time= 7.23322\n",
      "Epoch: 0304 train_loss= 1.09096 train_acc= 1.00000 test_acc= 0.68800 time= 7.25296\n",
      "Epoch: 0305 train_loss= 1.10093 train_acc= 0.99167 test_acc= 0.68600 time= 7.27225\n",
      "Epoch: 0306 train_loss= 1.09366 train_acc= 0.99167 test_acc= 0.68400 time= 7.29273\n",
      "Epoch: 0307 train_loss= 1.10387 train_acc= 0.98333 test_acc= 0.68400 time= 7.31399\n",
      "Epoch: 0308 train_loss= 1.09233 train_acc= 0.99167 test_acc= 0.68400 time= 7.33535\n",
      "Epoch: 0309 train_loss= 1.08882 train_acc= 1.00000 test_acc= 0.68700 time= 7.35628\n",
      "Epoch: 0310 train_loss= 1.09678 train_acc= 0.98333 test_acc= 0.68700 time= 7.37680\n",
      "Epoch: 0311 train_loss= 1.08732 train_acc= 1.00000 test_acc= 0.68500 time= 7.40379\n",
      "Epoch: 0312 train_loss= 1.09143 train_acc= 1.00000 test_acc= 0.68600 time= 7.42661\n",
      "Epoch: 0313 train_loss= 1.09799 train_acc= 0.99167 test_acc= 0.68700 time= 7.45477\n",
      "Epoch: 0314 train_loss= 1.09488 train_acc= 0.99167 test_acc= 0.68800 time= 7.47505\n",
      "Epoch: 0315 train_loss= 1.09109 train_acc= 0.99167 test_acc= 0.68500 time= 7.49455\n",
      "Epoch: 0316 train_loss= 1.10218 train_acc= 0.99167 test_acc= 0.68300 time= 7.51416\n",
      "Epoch: 0317 train_loss= 1.09467 train_acc= 0.99167 test_acc= 0.68300 time= 7.53388\n",
      "Epoch: 0318 train_loss= 1.08999 train_acc= 0.99167 test_acc= 0.68100 time= 7.55399\n",
      "Epoch: 0319 train_loss= 1.09262 train_acc= 1.00000 test_acc= 0.68500 time= 7.57424\n",
      "Epoch: 0320 train_loss= 1.09692 train_acc= 0.99167 test_acc= 0.68600 time= 7.60803\n",
      "Epoch: 0321 train_loss= 1.08999 train_acc= 1.00000 test_acc= 0.68600 time= 7.63649\n",
      "Epoch: 0322 train_loss= 1.09646 train_acc= 0.99167 test_acc= 0.68600 time= 7.66481\n",
      "Epoch: 0323 train_loss= 1.09648 train_acc= 0.99167 test_acc= 0.68600 time= 7.68594\n",
      "Epoch: 0324 train_loss= 1.09023 train_acc= 1.00000 test_acc= 0.68600 time= 7.70586\n",
      "Epoch: 0325 train_loss= 1.09900 train_acc= 0.99167 test_acc= 0.68600 time= 7.72570\n",
      "Epoch: 0326 train_loss= 1.09017 train_acc= 0.99167 test_acc= 0.68800 time= 7.74542\n",
      "Epoch: 0327 train_loss= 1.09819 train_acc= 0.99167 test_acc= 0.69300 time= 7.76490\n",
      "Epoch: 0328 train_loss= 1.09241 train_acc= 0.99167 test_acc= 0.69700 time= 7.78541\n",
      "Epoch: 0329 train_loss= 1.09321 train_acc= 1.00000 test_acc= 0.69500 time= 7.80816\n",
      "Epoch: 0330 train_loss= 1.09005 train_acc= 0.99167 test_acc= 0.69700 time= 7.82823\n",
      "Epoch: 0331 train_loss= 1.08758 train_acc= 1.00000 test_acc= 0.69700 time= 7.84802\n",
      "Epoch: 0332 train_loss= 1.08949 train_acc= 1.00000 test_acc= 0.69600 time= 7.86816\n",
      "Epoch: 0333 train_loss= 1.08749 train_acc= 1.00000 test_acc= 0.69600 time= 7.88912\n",
      "Epoch: 0334 train_loss= 1.09398 train_acc= 0.98333 test_acc= 0.69900 time= 7.91980\n",
      "Epoch: 0335 train_loss= 1.08670 train_acc= 1.00000 test_acc= 0.69500 time= 7.94378\n",
      "Epoch: 0336 train_loss= 1.09258 train_acc= 0.99167 test_acc= 0.69300 time= 7.96511\n",
      "Epoch: 0337 train_loss= 1.09161 train_acc= 1.00000 test_acc= 0.69400 time= 7.98561\n",
      "Epoch: 0338 train_loss= 1.09666 train_acc= 0.99167 test_acc= 0.69300 time= 8.00864\n",
      "Epoch: 0339 train_loss= 1.09018 train_acc= 0.99167 test_acc= 0.69000 time= 8.04902\n",
      "Epoch: 0340 train_loss= 1.08554 train_acc= 1.00000 test_acc= 0.69000 time= 8.07775\n",
      "Epoch: 0341 train_loss= 1.09269 train_acc= 0.98333 test_acc= 0.68800 time= 8.12253\n",
      "Epoch: 0342 train_loss= 1.08659 train_acc= 0.99167 test_acc= 0.68800 time= 8.17248\n",
      "Epoch: 0343 train_loss= 1.09498 train_acc= 0.99167 test_acc= 0.68800 time= 8.20029\n",
      "Epoch: 0344 train_loss= 1.08891 train_acc= 1.00000 test_acc= 0.68700 time= 8.22715\n",
      "Epoch: 0345 train_loss= 1.08932 train_acc= 0.99167 test_acc= 0.68800 time= 8.24878\n",
      "Epoch: 0346 train_loss= 1.09441 train_acc= 0.99167 test_acc= 0.68800 time= 8.27061\n",
      "Epoch: 0347 train_loss= 1.09330 train_acc= 0.99167 test_acc= 0.68700 time= 8.29101\n",
      "Epoch: 0348 train_loss= 1.08562 train_acc= 0.99167 test_acc= 0.68600 time= 8.31093\n",
      "Epoch: 0349 train_loss= 1.09269 train_acc= 0.99167 test_acc= 0.68700 time= 8.33377\n",
      "Epoch: 0350 train_loss= 1.09149 train_acc= 1.00000 test_acc= 0.68400 time= 8.35468\n",
      "Epoch: 0351 train_loss= 1.09731 train_acc= 0.99167 test_acc= 0.68400 time= 8.37478\n",
      "Epoch: 0352 train_loss= 1.08925 train_acc= 0.99167 test_acc= 0.67900 time= 8.39468\n",
      "Epoch: 0353 train_loss= 1.08955 train_acc= 0.99167 test_acc= 0.67800 time= 8.41568\n",
      "Epoch: 0354 train_loss= 1.08550 train_acc= 1.00000 test_acc= 0.67700 time= 8.43591\n",
      "Epoch: 0355 train_loss= 1.09016 train_acc= 0.99167 test_acc= 0.67900 time= 8.45802\n",
      "Epoch: 0356 train_loss= 1.09254 train_acc= 1.00000 test_acc= 0.68300 time= 8.47900\n",
      "Epoch: 0357 train_loss= 1.08770 train_acc= 0.99167 test_acc= 0.68500 time= 8.49921\n",
      "Epoch: 0358 train_loss= 1.09075 train_acc= 0.99167 test_acc= 0.68500 time= 8.51988\n",
      "Epoch: 0359 train_loss= 1.09199 train_acc= 1.00000 test_acc= 0.68600 time= 8.54023\n",
      "Epoch: 0360 train_loss= 1.09518 train_acc= 0.99167 test_acc= 0.68600 time= 8.56165\n",
      "Epoch: 0361 train_loss= 1.09524 train_acc= 0.99167 test_acc= 0.69000 time= 8.58363\n",
      "Epoch: 0362 train_loss= 1.08775 train_acc= 1.00000 test_acc= 0.69100 time= 8.60437\n",
      "Epoch: 0363 train_loss= 1.09439 train_acc= 0.99167 test_acc= 0.69300 time= 8.62488\n",
      "Epoch: 0364 train_loss= 1.08855 train_acc= 1.00000 test_acc= 0.69000 time= 8.64537\n",
      "Epoch: 0365 train_loss= 1.08140 train_acc= 1.00000 test_acc= 0.68900 time= 8.66890\n",
      "Epoch: 0366 train_loss= 1.08494 train_acc= 1.00000 test_acc= 0.68900 time= 8.69965\n",
      "Epoch: 0367 train_loss= 1.09357 train_acc= 0.99167 test_acc= 0.69000 time= 8.72431\n",
      "Epoch: 0368 train_loss= 1.08604 train_acc= 1.00000 test_acc= 0.69300 time= 8.74824\n",
      "Epoch: 0369 train_loss= 1.09111 train_acc= 0.99167 test_acc= 0.69300 time= 8.77681\n",
      "Epoch: 0370 train_loss= 1.08697 train_acc= 0.99167 test_acc= 0.69200 time= 8.80679\n",
      "Epoch: 0371 train_loss= 1.10587 train_acc= 0.98333 test_acc= 0.69400 time= 8.83281\n",
      "Epoch: 0372 train_loss= 1.08750 train_acc= 1.00000 test_acc= 0.69500 time= 8.85468\n",
      "Epoch: 0373 train_loss= 1.09136 train_acc= 0.99167 test_acc= 0.69500 time= 8.87632\n",
      "Epoch: 0374 train_loss= 1.08894 train_acc= 0.99167 test_acc= 0.69300 time= 8.89700\n",
      "Epoch: 0375 train_loss= 1.08841 train_acc= 1.00000 test_acc= 0.69500 time= 8.91856\n",
      "Epoch: 0376 train_loss= 1.09541 train_acc= 1.00000 test_acc= 0.69300 time= 8.93951\n",
      "Epoch: 0377 train_loss= 1.09209 train_acc= 0.99167 test_acc= 0.68800 time= 8.96145\n",
      "Epoch: 0378 train_loss= 1.08782 train_acc= 1.00000 test_acc= 0.68700 time= 8.99162\n",
      "Epoch: 0379 train_loss= 1.09180 train_acc= 0.99167 test_acc= 0.68700 time= 9.03276\n",
      "Epoch: 0380 train_loss= 1.09133 train_acc= 1.00000 test_acc= 0.68700 time= 9.07394\n",
      "Epoch: 0381 train_loss= 1.09091 train_acc= 0.99167 test_acc= 0.68700 time= 9.09606\n",
      "Epoch: 0382 train_loss= 1.09724 train_acc= 0.98333 test_acc= 0.68900 time= 9.11941\n",
      "Epoch: 0383 train_loss= 1.08252 train_acc= 1.00000 test_acc= 0.68300 time= 9.13983\n",
      "Epoch: 0384 train_loss= 1.09197 train_acc= 1.00000 test_acc= 0.69000 time= 9.15992\n",
      "Epoch: 0385 train_loss= 1.08545 train_acc= 1.00000 test_acc= 0.69000 time= 9.17982\n",
      "Epoch: 0386 train_loss= 1.08836 train_acc= 1.00000 test_acc= 0.69000 time= 9.20351\n",
      "Epoch: 0387 train_loss= 1.09007 train_acc= 1.00000 test_acc= 0.69100 time= 9.22564\n",
      "Epoch: 0388 train_loss= 1.08745 train_acc= 1.00000 test_acc= 0.69100 time= 9.24609\n",
      "Epoch: 0389 train_loss= 1.08376 train_acc= 1.00000 test_acc= 0.69100 time= 9.26628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0390 train_loss= 1.08923 train_acc= 0.99167 test_acc= 0.69100 time= 9.28690\n",
      "Epoch: 0391 train_loss= 1.08672 train_acc= 0.99167 test_acc= 0.69000 time= 9.31458\n",
      "Epoch: 0392 train_loss= 1.08884 train_acc= 1.00000 test_acc= 0.68600 time= 9.34885\n",
      "Epoch: 0393 train_loss= 1.08435 train_acc= 1.00000 test_acc= 0.68400 time= 9.37202\n",
      "Epoch: 0394 train_loss= 1.08292 train_acc= 1.00000 test_acc= 0.68200 time= 9.39242\n",
      "Epoch: 0395 train_loss= 1.08579 train_acc= 1.00000 test_acc= 0.68400 time= 9.41267\n",
      "Epoch: 0396 train_loss= 1.08769 train_acc= 0.99167 test_acc= 0.68200 time= 9.44309\n",
      "Epoch: 0397 train_loss= 1.08515 train_acc= 1.00000 test_acc= 0.68000 time= 9.47377\n",
      "Epoch: 0398 train_loss= 1.09683 train_acc= 0.99167 test_acc= 0.68000 time= 9.50672\n",
      "Epoch: 0399 train_loss= 1.08609 train_acc= 0.99167 test_acc= 0.67900 time= 9.55841\n",
      "Epoch: 0400 train_loss= 1.08493 train_acc= 1.00000 test_acc= 0.68100 time= 9.59407\n"
     ]
    }
   ],
   "source": [
    "# Set random seed\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "train_ratio = 120\n",
    "dataset = configs.FILES.citeseer\n",
    "lrate_gcn = configs.FILES.learning_rate\n",
    "print(\"train_ratio:\",train_ratio)\n",
    "\n",
    "x, _, adj_norm, labels, train_indexes, test_indexes = data_process.load_data(dataset, str(train_ratio), \n",
    "                                                                                x_flag='feature')\n",
    "\n",
    "node_num = adj_norm.shape[0]\n",
    "label_num = labels.shape[1]\n",
    "\n",
    "adj_norm_tuple = sparse.sparse_to_tuple(scipy.sparse.coo_matrix(adj_norm))\n",
    "feat_x_nn_tuple = sparse.sparse_to_tuple(scipy.sparse.coo_matrix(x))\n",
    "\n",
    "print(feat_x_nn_tuple)\n",
    "\n",
    "# node-node network train and validate masks\n",
    "nn_train_mask = np.zeros([node_num,])\n",
    "nn_test_mask = np.zeros([node_num,])\n",
    "\n",
    "for i in train_indexes:\n",
    "    nn_train_mask[i] = 1\n",
    "    nn_test_mask[i] = 0\n",
    "    \n",
    "for i in test_indexes:\n",
    "    nn_test_mask[i] = 1\n",
    "    \n",
    "# TensorFlow placeholders\n",
    "ph = {\n",
    "      'adj_norm': tf.sparse_placeholder(tf.float32, name=\"adj_norm\"),\n",
    "      'x': tf.sparse_placeholder(tf.float32, name=\"features\"),\n",
    "      'labels': tf.placeholder(tf.float32, name=\"node_labels\"),\n",
    "      'mask': tf.placeholder(tf.int32, shape=(node_num,))\n",
    "      }\n",
    "\n",
    "placeholders = {\n",
    "                'dropout_prob': tf.placeholder_with_default(0., shape=()),\n",
    "                'num_features_nonzero': tf.placeholder(tf.int32)\n",
    "                }\n",
    "\n",
    "\n",
    "# the first layer\n",
    "# feat_embeds, proj_X = feat2struct.BuildGraphStruct(out_size = 10,\n",
    "#                                         dropout_prob = 0.3,\n",
    "#                                         act = tf.nn.relu,\n",
    "#                                         feat_dropout = True)(X = x.toarray())\n",
    "\n",
    "t_model = mg.GraphConvLayer(input_dim=x.shape[-1],\n",
    "                           output_dim=configs.FILES.h,\n",
    "                           name='nn_fc1',\n",
    "                           holders=placeholders,\n",
    "                           act=tf.nn.relu,\n",
    "                           dropout=True)\n",
    "\n",
    "nn_fc1 = t_model(adj_norm=ph['adj_norm'],\n",
    "                           x=ph['x'], sparse=True)\n",
    "                            \n",
    "\n",
    "# the second layer\n",
    "nn_dl = mg.GraphConvLayer(input_dim=configs.FILES.h,\n",
    "                           output_dim=label_num,\n",
    "                           name='nn_dl',\n",
    "                           holders=placeholders,\n",
    "                           act=tf.nn.softmax,\n",
    "                           dropout=True)(adj_norm=ph['adj_norm'],\n",
    "                                           x=nn_fc1)  \n",
    "\n",
    "\n",
    "def masked_sigmoid_softmax_cross_entropy(preds, labels, mask):\n",
    "    \"\"\"Sigmoid softmax cross-entropy loss with masking.\"\"\"\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=preds, labels=labels)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    mask /= tf.reduce_mean(mask)\n",
    "    loss *= mask\n",
    "    for var in t_model.var.values():\n",
    "        loss += configs.FILES.weight_decay * tf.nn.l2_loss(var)\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "def masked_accuracy(preds, labels, mask):\n",
    "    \"\"\"Accuracy with masking.\"\"\"\n",
    "    correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(labels, 1))\n",
    "    accuracy_all = tf.cast(correct_prediction, tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    mask /= tf.reduce_mean(mask)\n",
    "    accuracy_all *= mask\n",
    "    \n",
    "    return tf.reduce_mean(accuracy_all), correct_prediction\n",
    "\n",
    "# calculate the classification accuracy per classes   \n",
    "def precision_per_class(preds, labels, mask):\n",
    "    import heapq\n",
    "    mask = mask.astype(int)\n",
    "    labels = labels.astype(int)\n",
    "    val_indexes = np.where(mask==1)[0]\n",
    "    pred_true_labels = {}\n",
    "    \n",
    "    y_true = []\n",
    "    y_pred = []    \n",
    "    \n",
    "    for i in val_indexes:\n",
    "        pred_probs_i = preds[i]\n",
    "        true_raw_i = labels[i]\n",
    "        \n",
    "        pred_label_i = heapq.nlargest(np.sum(true_raw_i),range(len(pred_probs_i)), \n",
    "                                      pred_probs_i.take)\n",
    "        true_label_i = np.where(true_raw_i==1)[0]\n",
    "        pred_true_labels[i] = (pred_label_i, true_label_i)\n",
    "        \n",
    "        y_true.append(true_label_i)\n",
    "        y_pred.append(pred_label_i)\n",
    "        \n",
    "    accuracy_per_classes = metrics.evaluate(pred_true_labels)\n",
    "    \n",
    "    from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
    "    \n",
    "    mat = confusion_matrix(y_true, y_pred)\n",
    "    print(mat)        \n",
    "        \n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    test_y = labels[val_indexes]\n",
    "    test_pred = preds[val_indexes]\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(test_y.ravel(), test_pred.ravel())\n",
    "    auc = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "    \n",
    "    return accuracy_per_classes, auc\n",
    "    \n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    test_y = labels[val_indexes]\n",
    "    test_pred = preds[val_indexes]\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(test_y.ravel(), test_pred.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "    print('micro_auc=',roc_auc[\"micro\"])\n",
    "    \n",
    "    return accuracy_per_classes\n",
    "\n",
    "with tf.name_scope('optimizer'):\n",
    "    \n",
    "    loss = masked_sigmoid_softmax_cross_entropy(preds=nn_dl, \n",
    "                                                labels=ph['labels'], mask=ph['mask'])\n",
    "        \n",
    "    accuracy, correct_prediction = masked_accuracy(preds=nn_dl, \n",
    "                               labels=ph['labels'], mask=ph['mask'])\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=lrate_gcn)\n",
    "    opt_op = optimizer.minimize(loss)    \n",
    "\n",
    "feed_dict_train = {ph['adj_norm']: adj_norm_tuple,\n",
    "                      ph['x']: feat_x_nn_tuple,\n",
    "#                     ph['labels']: labels,\n",
    "                    ph['labels']: labels.toarray(),\n",
    "                      ph['mask']: nn_train_mask,\n",
    "                      placeholders['dropout_prob']: configs.FILES.dropout_prob,\n",
    "                      placeholders['num_features_nonzero']: feat_x_nn_tuple[1].shape,\n",
    "                      }\n",
    "feed_dict_val = {ph['adj_norm']: adj_norm_tuple,\n",
    "                    ph['x']: feat_x_nn_tuple,\n",
    "#                     ph['labels']: labels,\n",
    "                    ph['labels']: labels.toarray(),\n",
    "                    ph['mask']: nn_test_mask,\n",
    "                    placeholders['dropout_prob']: 0.,\n",
    "                    placeholders['num_features_nonzero']: feat_x_nn_tuple[1].shape\n",
    "                    }\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "epochs = 400\n",
    "save_every = 1    \n",
    "    \n",
    "t = time.time()\n",
    "times = []\n",
    "# Train model\n",
    "for epoch in range(epochs):\n",
    "    # Training node embedding\n",
    "#     _, train_loss = sess.run(\n",
    "#         (opt_op, loss), feed_dict=feed_dict_train)\n",
    "    _, train_loss, train_acc, train_nn_dl = sess.run((opt_op, loss, accuracy, nn_dl), \n",
    "                                                              feed_dict=feed_dict_train)\n",
    "    \n",
    "    if epoch % save_every == 0:\n",
    "        val_acc, test_nn_dl = sess.run((accuracy, nn_dl), feed_dict=feed_dict_val)\n",
    "        \n",
    "        print(\"Epoch:\", '%04d' % (epoch + 1),\n",
    "              \"train_loss=\", \"{:.5f}\".format(train_loss),\n",
    "              \"train_acc=\", \"{:.5f}\".format(train_acc),\n",
    "#               \"train_auc\", \"{:.5f}\".format(train_auc),\n",
    "              \"test_acc=\", \"{:.5f}\".format(val_acc),\n",
    "#               \"test_auc\", \"{:.5f}\".format(test_auc),\n",
    "              \"time=\", \"{:.5f}\".format(time.time() - t))\n",
    "    \n",
    "    if epoch % 20 == 0:\n",
    "        times.append(time.time() - t)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
