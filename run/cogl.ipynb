{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "\n",
    "import time\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior() \n",
    "import numpy as np\n",
    "import models.graph as mg\n",
    "import models.adversarialNets as ma\n",
    "import scipy.sparse\n",
    "from utils import data_process, sparse\n",
    "from utils import configs, metrics, feat2struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: 800\n",
      "train_ratio: 120\n",
      "label_distribution: [('0', 20), ('1', 20), ('2', 20), ('3', 20), ('4', 20), ('5', 20)]\n",
      "balance_num: 20\n",
      "(3312, 3703) (3312, 3312) (3312, 6) (120,) (1000,)\n",
      "WARNING:tensorflow:From /home/mins/.local/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:201: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/mins/DLearn-GCN/utils/feat2struct.py:32: conv1d (from tensorflow.python.keras.legacy_tf_layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv1D` instead.\n",
      "WARNING:tensorflow:From /home/mins/.local/lib/python3.8/site-packages/tensorflow/python/keras/legacy_tf_layers/convolutional.py:218: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From /home/mins/.local/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:201: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/mins/.local/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:201: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mins/.local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:432: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 dd_loss= 1.38209 gg_loss= 0.69872 cclass_loss= 1.80074 train_acc= 0.25000 test_acc= 0.12400 time= 14.95932\n",
      "Epoch: 0002 dd_loss= 1.38677 gg_loss= 0.67778 cclass_loss= 1.79252 train_acc= 0.35000 test_acc= 0.27200 time= 22.86595\n",
      "Epoch: 0003 dd_loss= 1.38750 gg_loss= 0.65962 cclass_loss= 1.78169 train_acc= 0.52500 test_acc= 0.31200 time= 30.01443\n",
      "Epoch: 0004 dd_loss= 1.38971 gg_loss= 0.64379 cclass_loss= 1.76973 train_acc= 0.64167 test_acc= 0.37400 time= 36.91939\n",
      "Epoch: 0005 dd_loss= 1.39048 gg_loss= 0.63052 cclass_loss= 1.75416 train_acc= 0.75000 test_acc= 0.42600 time= 43.94264\n",
      "Epoch: 0006 dd_loss= 1.39336 gg_loss= 0.61664 cclass_loss= 1.73740 train_acc= 0.85833 test_acc= 0.46400 time= 51.79827\n",
      "Epoch: 0007 dd_loss= 1.39673 gg_loss= 0.60323 cclass_loss= 1.71679 train_acc= 0.84167 test_acc= 0.51800 time= 60.35088\n",
      "Epoch: 0008 dd_loss= 1.39937 gg_loss= 0.59114 cclass_loss= 1.70336 train_acc= 0.85833 test_acc= 0.53800 time= 68.29365\n",
      "Epoch: 0009 dd_loss= 1.39781 gg_loss= 0.58300 cclass_loss= 1.67603 train_acc= 0.85833 test_acc= 0.56200 time= 75.90483\n",
      "Epoch: 0010 dd_loss= 1.40264 gg_loss= 0.57104 cclass_loss= 1.63755 train_acc= 0.90000 test_acc= 0.57600 time= 82.88938\n",
      "Epoch: 0011 dd_loss= 1.40580 gg_loss= 0.56064 cclass_loss= 1.60687 train_acc= 0.91667 test_acc= 0.59300 time= 90.27905\n",
      "Epoch: 0012 dd_loss= 1.40968 gg_loss= 0.55071 cclass_loss= 1.57997 train_acc= 0.92500 test_acc= 0.61500 time= 98.02347\n",
      "Epoch: 0013 dd_loss= 1.40606 gg_loss= 0.54661 cclass_loss= 1.53326 train_acc= 0.94167 test_acc= 0.64400 time= 105.01671\n",
      "Epoch: 0014 dd_loss= 1.40742 gg_loss= 0.53964 cclass_loss= 1.51341 train_acc= 0.93333 test_acc= 0.64600 time= 112.94074\n",
      "Epoch: 0015 dd_loss= 1.41252 gg_loss= 0.52924 cclass_loss= 1.47700 train_acc= 0.96667 test_acc= 0.65100 time= 119.61913\n",
      "Epoch: 0016 dd_loss= 1.41459 gg_loss= 0.52306 cclass_loss= 1.45884 train_acc= 0.94167 test_acc= 0.66500 time= 125.86032\n",
      "Epoch: 0017 dd_loss= 1.41114 gg_loss= 0.51986 cclass_loss= 1.42277 train_acc= 0.94167 test_acc= 0.67200 time= 131.97463\n",
      "Epoch: 0018 dd_loss= 1.40544 gg_loss= 0.51828 cclass_loss= 1.38349 train_acc= 0.96667 test_acc= 0.67600 time= 139.26421\n",
      "Epoch: 0019 dd_loss= 1.41002 gg_loss= 0.50898 cclass_loss= 1.35172 train_acc= 0.95000 test_acc= 0.68300 time= 147.11899\n",
      "Epoch: 0020 dd_loss= 1.41683 gg_loss= 0.50293 cclass_loss= 1.36113 train_acc= 0.96667 test_acc= 0.68500 time= 155.06422\n",
      "Epoch: 0021 dd_loss= 1.41229 gg_loss= 0.50110 cclass_loss= 1.33412 train_acc= 0.95833 test_acc= 0.68300 time= 162.69635\n",
      "Epoch: 0022 dd_loss= 1.40625 gg_loss= 0.50252 cclass_loss= 1.33104 train_acc= 0.97500 test_acc= 0.68300 time= 169.49503\n",
      "Epoch: 0023 dd_loss= 1.41298 gg_loss= 0.49427 cclass_loss= 1.30051 train_acc= 0.96667 test_acc= 0.68800 time= 177.81351\n",
      "Epoch: 0024 dd_loss= 1.41086 gg_loss= 0.49208 cclass_loss= 1.29418 train_acc= 0.98333 test_acc= 0.68800 time= 186.07063\n",
      "Epoch: 0025 dd_loss= 1.41446 gg_loss= 0.48651 cclass_loss= 1.26849 train_acc= 0.95833 test_acc= 0.68900 time= 194.03934\n",
      "Epoch: 0026 dd_loss= 1.39961 gg_loss= 0.49317 cclass_loss= 1.25964 train_acc= 0.97500 test_acc= 0.68700 time= 201.16493\n",
      "Epoch: 0027 dd_loss= 1.40888 gg_loss= 0.48387 cclass_loss= 1.25336 train_acc= 0.96667 test_acc= 0.68700 time= 208.16918\n",
      "Epoch: 0028 dd_loss= 1.40873 gg_loss= 0.48108 cclass_loss= 1.25391 train_acc= 0.95833 test_acc= 0.69200 time= 215.32445\n",
      "Epoch: 0029 dd_loss= 1.41499 gg_loss= 0.47580 cclass_loss= 1.25659 train_acc= 0.97500 test_acc= 0.69100 time= 222.65234\n",
      "Epoch: 0030 dd_loss= 1.39232 gg_loss= 0.48701 cclass_loss= 1.21963 train_acc= 1.00000 test_acc= 0.68900 time= 230.27092\n",
      "Epoch: 0031 dd_loss= 1.40166 gg_loss= 0.47928 cclass_loss= 1.23647 train_acc= 0.97500 test_acc= 0.69600 time= 237.92323\n",
      "Epoch: 0032 dd_loss= 1.40833 gg_loss= 0.47268 cclass_loss= 1.22433 train_acc= 0.99167 test_acc= 0.69100 time= 245.89363\n",
      "Epoch: 0033 dd_loss= 1.41072 gg_loss= 0.46909 cclass_loss= 1.22079 train_acc= 0.98333 test_acc= 0.68600 time= 253.40161\n",
      "Epoch: 0034 dd_loss= 1.39202 gg_loss= 0.47871 cclass_loss= 1.22176 train_acc= 0.98333 test_acc= 0.70000 time= 260.98170\n",
      "Epoch: 0035 dd_loss= 1.39415 gg_loss= 0.47462 cclass_loss= 1.22040 train_acc= 0.97500 test_acc= 0.69100 time= 269.19745\n",
      "Epoch: 0036 dd_loss= 1.40275 gg_loss= 0.46765 cclass_loss= 1.22024 train_acc= 0.97500 test_acc= 0.68800 time= 275.73579\n",
      "Epoch: 0037 dd_loss= 1.41031 gg_loss= 0.46155 cclass_loss= 1.20570 train_acc= 0.99167 test_acc= 0.69600 time= 283.91124\n",
      "Epoch: 0038 dd_loss= 1.38595 gg_loss= 0.47503 cclass_loss= 1.19515 train_acc= 0.99167 test_acc= 0.69600 time= 292.23578\n",
      "Epoch: 0039 dd_loss= 1.38814 gg_loss= 0.47081 cclass_loss= 1.21088 train_acc= 0.99167 test_acc= 0.69500 time= 298.61142\n",
      "Epoch: 0040 dd_loss= 1.39479 gg_loss= 0.46586 cclass_loss= 1.19186 train_acc= 0.98333 test_acc= 0.69400 time= 305.86647\n",
      "Epoch: 0041 dd_loss= 1.40159 gg_loss= 0.45994 cclass_loss= 1.18852 train_acc= 0.99167 test_acc= 0.69600 time= 312.46723\n",
      "Epoch: 0042 dd_loss= 1.37798 gg_loss= 0.47286 cclass_loss= 1.18988 train_acc= 0.98333 test_acc= 0.69600 time= 318.48461\n",
      "Epoch: 0043 dd_loss= 1.37704 gg_loss= 0.47364 cclass_loss= 1.18921 train_acc= 0.97500 test_acc= 0.69800 time= 324.70298\n",
      "Epoch: 0044 dd_loss= 1.38712 gg_loss= 0.46556 cclass_loss= 1.18299 train_acc= 0.99167 test_acc= 0.69600 time= 332.47420\n",
      "Epoch: 0045 dd_loss= 1.39266 gg_loss= 0.46019 cclass_loss= 1.18462 train_acc= 0.99167 test_acc= 0.69800 time= 340.60735\n",
      "Epoch: 0046 dd_loss= 1.38279 gg_loss= 0.46790 cclass_loss= 1.18257 train_acc= 0.99167 test_acc= 0.70200 time= 347.70671\n",
      "Epoch: 0047 dd_loss= 1.36643 gg_loss= 0.47492 cclass_loss= 1.18185 train_acc= 0.99167 test_acc= 0.70000 time= 353.88702\n",
      "Epoch: 0048 dd_loss= 1.38181 gg_loss= 0.46252 cclass_loss= 1.19882 train_acc= 0.97500 test_acc= 0.69600 time= 359.81963\n",
      "Epoch: 0049 dd_loss= 1.38916 gg_loss= 0.45998 cclass_loss= 1.17206 train_acc= 1.00000 test_acc= 0.70000 time= 365.91008\n",
      "Epoch: 0050 dd_loss= 1.38398 gg_loss= 0.45981 cclass_loss= 1.17699 train_acc= 0.99167 test_acc= 0.70100 time= 372.71530\n",
      "Epoch: 0051 dd_loss= 1.34600 gg_loss= 0.48147 cclass_loss= 1.16759 train_acc= 0.99167 test_acc= 0.70400 time= 380.10585\n",
      "Epoch: 0052 dd_loss= 1.37916 gg_loss= 0.46138 cclass_loss= 1.16905 train_acc= 0.98333 test_acc= 0.70200 time= 389.19166\n",
      "Epoch: 0053 dd_loss= 1.37915 gg_loss= 0.45957 cclass_loss= 1.16597 train_acc= 0.99167 test_acc= 0.70000 time= 398.56232\n",
      "Epoch: 0054 dd_loss= 1.38224 gg_loss= 0.45612 cclass_loss= 1.16802 train_acc= 1.00000 test_acc= 0.70200 time= 408.49321\n",
      "Epoch: 0055 dd_loss= 1.34204 gg_loss= 0.47992 cclass_loss= 1.16280 train_acc= 1.00000 test_acc= 0.70300 time= 414.99881\n",
      "Epoch: 0056 dd_loss= 1.37159 gg_loss= 0.46278 cclass_loss= 1.16559 train_acc= 1.00000 test_acc= 0.70400 time= 420.41364\n",
      "Epoch: 0057 dd_loss= 1.37038 gg_loss= 0.46095 cclass_loss= 1.16651 train_acc= 1.00000 test_acc= 0.70000 time= 425.33987\n",
      "Epoch: 0058 dd_loss= 1.37630 gg_loss= 0.45611 cclass_loss= 1.16698 train_acc= 0.99167 test_acc= 0.69900 time= 430.32403\n",
      "Epoch: 0059 dd_loss= 1.33257 gg_loss= 0.48556 cclass_loss= 1.16061 train_acc= 0.99167 test_acc= 0.70900 time= 435.98143\n",
      "Epoch: 0060 dd_loss= 1.35943 gg_loss= 0.46511 cclass_loss= 1.15851 train_acc= 0.99167 test_acc= 0.70200 time= 442.13858\n",
      "Epoch: 0061 dd_loss= 1.36180 gg_loss= 0.46360 cclass_loss= 1.15951 train_acc= 0.99167 test_acc= 0.70400 time= 447.72076\n",
      "Epoch: 0062 dd_loss= 1.37737 gg_loss= 0.45392 cclass_loss= 1.15152 train_acc= 0.99167 test_acc= 0.70600 time= 453.63725\n",
      "Epoch: 0063 dd_loss= 1.33671 gg_loss= 0.47914 cclass_loss= 1.16205 train_acc= 0.99167 test_acc= 0.71100 time= 460.57485\n",
      "Epoch: 0064 dd_loss= 1.34438 gg_loss= 0.47159 cclass_loss= 1.17676 train_acc= 0.98333 test_acc= 0.69900 time= 468.38442\n",
      "Epoch: 0065 dd_loss= 1.35202 gg_loss= 0.46727 cclass_loss= 1.15415 train_acc= 1.00000 test_acc= 0.70400 time= 476.23226\n",
      "Epoch: 0066 dd_loss= 1.37174 gg_loss= 0.45353 cclass_loss= 1.15974 train_acc= 0.99167 test_acc= 0.70600 time= 484.48454\n",
      "Epoch: 0067 dd_loss= 1.32811 gg_loss= 0.48196 cclass_loss= 1.15632 train_acc= 1.00000 test_acc= 0.70500 time= 491.03838\n",
      "Epoch: 0068 dd_loss= 1.32168 gg_loss= 0.48246 cclass_loss= 1.15363 train_acc= 0.99167 test_acc= 0.70500 time= 497.20035\n",
      "Epoch: 0069 dd_loss= 1.34768 gg_loss= 0.46715 cclass_loss= 1.16667 train_acc= 0.99167 test_acc= 0.70500 time= 503.29853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0070 dd_loss= 1.36383 gg_loss= 0.45460 cclass_loss= 1.17094 train_acc= 0.99167 test_acc= 0.70200 time= 510.04650\n",
      "Epoch: 0071 dd_loss= 1.32032 gg_loss= 0.48423 cclass_loss= 1.16209 train_acc= 0.99167 test_acc= 0.70700 time= 516.18740\n",
      "Epoch: 0072 dd_loss= 1.31303 gg_loss= 0.48639 cclass_loss= 1.15002 train_acc= 1.00000 test_acc= 0.70400 time= 522.69266\n",
      "Epoch: 0073 dd_loss= 1.33441 gg_loss= 0.47174 cclass_loss= 1.15465 train_acc= 0.99167 test_acc= 0.70300 time= 529.20420\n",
      "Epoch: 0074 dd_loss= 1.35288 gg_loss= 0.45986 cclass_loss= 1.15373 train_acc= 0.99167 test_acc= 0.70000 time= 535.77229\n",
      "Epoch: 0075 dd_loss= 1.32614 gg_loss= 0.47575 cclass_loss= 1.16237 train_acc= 0.98333 test_acc= 0.70500 time= 542.01070\n",
      "Epoch: 0076 dd_loss= 1.29364 gg_loss= 0.49677 cclass_loss= 1.15993 train_acc= 0.99167 test_acc= 0.70800 time= 548.18898\n",
      "Epoch: 0077 dd_loss= 1.32602 gg_loss= 0.47334 cclass_loss= 1.16017 train_acc= 0.98333 test_acc= 0.70500 time= 554.34701\n",
      "Epoch: 0078 dd_loss= 1.33513 gg_loss= 0.46791 cclass_loss= 1.15820 train_acc= 1.00000 test_acc= 0.70200 time= 560.62081\n",
      "Epoch: 0079 dd_loss= 1.32060 gg_loss= 0.47806 cclass_loss= 1.15030 train_acc= 1.00000 test_acc= 0.70600 time= 566.58621\n",
      "Epoch: 0080 dd_loss= 1.27614 gg_loss= 0.50521 cclass_loss= 1.15981 train_acc= 0.99167 test_acc= 0.70800 time= 573.84032\n",
      "Epoch: 0081 dd_loss= 1.32434 gg_loss= 0.47203 cclass_loss= 1.16521 train_acc= 0.99167 test_acc= 0.70700 time= 581.03051\n",
      "Epoch: 0082 dd_loss= 1.32008 gg_loss= 0.47356 cclass_loss= 1.15311 train_acc= 1.00000 test_acc= 0.70200 time= 587.83524\n",
      "Epoch: 0083 dd_loss= 1.32169 gg_loss= 0.47283 cclass_loss= 1.14420 train_acc= 0.99167 test_acc= 0.70300 time= 594.84428\n",
      "Epoch: 0084 dd_loss= 1.27289 gg_loss= 0.50627 cclass_loss= 1.14761 train_acc= 0.99167 test_acc= 0.70400 time= 602.01747\n",
      "Epoch: 0085 dd_loss= 1.30608 gg_loss= 0.48145 cclass_loss= 1.14757 train_acc= 0.99167 test_acc= 0.70200 time= 608.97600\n",
      "Epoch: 0086 dd_loss= 1.31794 gg_loss= 0.47542 cclass_loss= 1.15024 train_acc= 0.98333 test_acc= 0.70200 time= 615.82210\n",
      "Epoch: 0087 dd_loss= 1.32073 gg_loss= 0.47095 cclass_loss= 1.14325 train_acc= 0.99167 test_acc= 0.70700 time= 622.43184\n",
      "Epoch: 0088 dd_loss= 1.27924 gg_loss= 0.50012 cclass_loss= 1.14899 train_acc= 0.99167 test_acc= 0.70000 time= 629.31799\n",
      "Epoch: 0089 dd_loss= 1.28901 gg_loss= 0.49022 cclass_loss= 1.14840 train_acc= 0.99167 test_acc= 0.70200 time= 635.93984\n",
      "Epoch: 0090 dd_loss= 1.30169 gg_loss= 0.48252 cclass_loss= 1.14124 train_acc= 1.00000 test_acc= 0.70800 time= 642.65012\n",
      "Epoch: 0091 dd_loss= 1.32817 gg_loss= 0.46653 cclass_loss= 1.14369 train_acc= 0.99167 test_acc= 0.70600 time= 648.16828\n",
      "Epoch: 0092 dd_loss= 1.26787 gg_loss= 0.50436 cclass_loss= 1.14273 train_acc= 0.99167 test_acc= 0.70400 time= 652.57415\n",
      "Epoch: 0093 dd_loss= 1.28611 gg_loss= 0.49178 cclass_loss= 1.13852 train_acc= 1.00000 test_acc= 0.70000 time= 656.70854\n",
      "Epoch: 0094 dd_loss= 1.29176 gg_loss= 0.48728 cclass_loss= 1.14530 train_acc= 0.99167 test_acc= 0.70300 time= 660.90031\n",
      "Epoch: 0095 dd_loss= 1.31802 gg_loss= 0.46949 cclass_loss= 1.14466 train_acc= 1.00000 test_acc= 0.70800 time= 665.00288\n",
      "Epoch: 0096 dd_loss= 1.25380 gg_loss= 0.51469 cclass_loss= 1.14336 train_acc= 1.00000 test_acc= 0.71200 time= 669.00491\n",
      "Epoch: 0097 dd_loss= 1.26569 gg_loss= 0.50247 cclass_loss= 1.15187 train_acc= 0.98333 test_acc= 0.70500 time= 673.16582\n",
      "Epoch: 0098 dd_loss= 1.28856 gg_loss= 0.48690 cclass_loss= 1.14529 train_acc= 1.00000 test_acc= 0.70600 time= 677.35775\n",
      "Epoch: 0099 dd_loss= 1.31033 gg_loss= 0.47109 cclass_loss= 1.15096 train_acc= 0.99167 test_acc= 0.70100 time= 681.54325\n",
      "Epoch: 0100 dd_loss= 1.26096 gg_loss= 0.50610 cclass_loss= 1.15089 train_acc= 0.98333 test_acc= 0.70800 time= 685.81583\n",
      "Epoch: 0101 dd_loss= 1.25133 gg_loss= 0.51058 cclass_loss= 1.14143 train_acc= 1.00000 test_acc= 0.70900 time= 690.08233\n",
      "Epoch: 0102 dd_loss= 1.26430 gg_loss= 0.49754 cclass_loss= 1.16017 train_acc= 0.97500 test_acc= 0.70500 time= 694.23651\n",
      "Epoch: 0103 dd_loss= 1.30426 gg_loss= 0.47266 cclass_loss= 1.14967 train_acc= 0.99167 test_acc= 0.70300 time= 698.31789\n",
      "Epoch: 0104 dd_loss= 1.26008 gg_loss= 0.50252 cclass_loss= 1.13748 train_acc= 1.00000 test_acc= 0.70700 time= 702.87481\n",
      "Epoch: 0105 dd_loss= 1.22667 gg_loss= 0.52410 cclass_loss= 1.14152 train_acc= 0.99167 test_acc= 0.71000 time= 707.38009\n",
      "Epoch: 0106 dd_loss= 1.26652 gg_loss= 0.49358 cclass_loss= 1.14188 train_acc= 0.99167 test_acc= 0.70700 time= 711.87846\n",
      "Epoch: 0107 dd_loss= 1.28313 gg_loss= 0.48355 cclass_loss= 1.13859 train_acc= 0.99167 test_acc= 0.70400 time= 716.11697\n",
      "Epoch: 0108 dd_loss= 1.26417 gg_loss= 0.49650 cclass_loss= 1.14503 train_acc= 1.00000 test_acc= 0.70300 time= 720.59970\n",
      "Epoch: 0109 dd_loss= 1.22194 gg_loss= 0.52683 cclass_loss= 1.14139 train_acc= 1.00000 test_acc= 0.71300 time= 725.01618\n",
      "Epoch: 0110 dd_loss= 1.26649 gg_loss= 0.49256 cclass_loss= 1.13961 train_acc= 1.00000 test_acc= 0.71000 time= 729.35529\n",
      "Epoch: 0111 dd_loss= 1.26786 gg_loss= 0.49117 cclass_loss= 1.13398 train_acc= 1.00000 test_acc= 0.70500 time= 733.80934\n",
      "Epoch: 0112 dd_loss= 1.26328 gg_loss= 0.49456 cclass_loss= 1.14165 train_acc= 0.99167 test_acc= 0.70900 time= 738.23251\n",
      "Epoch: 0113 dd_loss= 1.20317 gg_loss= 0.53683 cclass_loss= 1.14776 train_acc= 1.00000 test_acc= 0.71300 time= 742.81894\n",
      "Epoch: 0114 dd_loss= 1.24705 gg_loss= 0.50512 cclass_loss= 1.15282 train_acc= 0.98333 test_acc= 0.70500 time= 747.24017\n",
      "Epoch: 0115 dd_loss= 1.25047 gg_loss= 0.50281 cclass_loss= 1.14139 train_acc= 0.99167 test_acc= 0.70500 time= 751.51260\n",
      "Epoch: 0116 dd_loss= 1.27166 gg_loss= 0.48732 cclass_loss= 1.15256 train_acc= 0.99167 test_acc= 0.70600 time= 755.97076\n",
      "Epoch: 0117 dd_loss= 1.20038 gg_loss= 0.53908 cclass_loss= 1.13744 train_acc= 1.00000 test_acc= 0.71200 time= 760.24019\n",
      "Epoch: 0118 dd_loss= 1.23966 gg_loss= 0.50690 cclass_loss= 1.13786 train_acc= 0.99167 test_acc= 0.70800 time= 764.57000\n",
      "Epoch: 0119 dd_loss= 1.24827 gg_loss= 0.50032 cclass_loss= 1.15184 train_acc= 0.98333 test_acc= 0.70600 time= 768.92424\n",
      "Epoch: 0120 dd_loss= 1.26417 gg_loss= 0.48916 cclass_loss= 1.14347 train_acc= 0.99167 test_acc= 0.70600 time= 773.29001\n",
      "Epoch: 0121 dd_loss= 1.19224 gg_loss= 0.54129 cclass_loss= 1.13231 train_acc= 0.99167 test_acc= 0.70900 time= 777.54576\n",
      "Epoch: 0122 dd_loss= 1.22481 gg_loss= 0.51406 cclass_loss= 1.13982 train_acc= 1.00000 test_acc= 0.71100 time= 781.98705\n",
      "Epoch: 0123 dd_loss= 1.23296 gg_loss= 0.50879 cclass_loss= 1.15146 train_acc= 0.99167 test_acc= 0.70900 time= 786.39925\n",
      "Epoch: 0124 dd_loss= 1.27388 gg_loss= 0.48098 cclass_loss= 1.14293 train_acc= 0.99167 test_acc= 0.70900 time= 790.63354\n",
      "Epoch: 0125 dd_loss= 1.19391 gg_loss= 0.53579 cclass_loss= 1.14049 train_acc= 0.99167 test_acc= 0.70400 time= 794.72834\n",
      "Epoch: 0126 dd_loss= 1.20616 gg_loss= 0.53031 cclass_loss= 1.13521 train_acc= 1.00000 test_acc= 0.71000 time= 799.23195\n",
      "Epoch: 0127 dd_loss= 1.21638 gg_loss= 0.51832 cclass_loss= 1.14005 train_acc= 0.99167 test_acc= 0.70900 time= 803.50852\n",
      "Epoch: 0128 dd_loss= 1.25888 gg_loss= 0.49221 cclass_loss= 1.13753 train_acc= 0.98333 test_acc= 0.71200 time= 807.82136\n",
      "Epoch: 0129 dd_loss= 1.20112 gg_loss= 0.52935 cclass_loss= 1.13677 train_acc= 0.99167 test_acc= 0.70800 time= 812.06936\n",
      "Epoch: 0130 dd_loss= 1.20793 gg_loss= 0.52501 cclass_loss= 1.13917 train_acc= 0.98333 test_acc= 0.70400 time= 815.95096\n",
      "Epoch: 0131 dd_loss= 1.21466 gg_loss= 0.51676 cclass_loss= 1.15414 train_acc= 0.99167 test_acc= 0.70600 time= 819.69408\n",
      "Epoch: 0132 dd_loss= 1.26161 gg_loss= 0.48547 cclass_loss= 1.13490 train_acc= 0.99167 test_acc= 0.71000 time= 823.52247\n",
      "Epoch: 0133 dd_loss= 1.22242 gg_loss= 0.51275 cclass_loss= 1.14387 train_acc= 0.99167 test_acc= 0.71100 time= 827.35119\n",
      "Epoch: 0134 dd_loss= 1.17757 gg_loss= 0.54445 cclass_loss= 1.13706 train_acc= 1.00000 test_acc= 0.70600 time= 831.11725\n",
      "Epoch: 0135 dd_loss= 1.20370 gg_loss= 0.52142 cclass_loss= 1.13081 train_acc= 0.99167 test_acc= 0.70400 time= 835.15184\n",
      "Epoch: 0136 dd_loss= 1.22832 gg_loss= 0.50622 cclass_loss= 1.13221 train_acc= 1.00000 test_acc= 0.70900 time= 839.19807\n",
      "Epoch: 0137 dd_loss= 1.22683 gg_loss= 0.50868 cclass_loss= 1.13615 train_acc= 0.99167 test_acc= 0.70700 time= 843.35619\n",
      "Epoch: 0138 dd_loss= 1.16498 gg_loss= 0.55110 cclass_loss= 1.12350 train_acc= 1.00000 test_acc= 0.70500 time= 847.31165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0139 dd_loss= 1.21701 gg_loss= 0.51345 cclass_loss= 1.14436 train_acc= 0.99167 test_acc= 0.71100 time= 851.45859\n",
      "Epoch: 0140 dd_loss= 1.21701 gg_loss= 0.51212 cclass_loss= 1.14134 train_acc= 0.99167 test_acc= 0.70800 time= 857.30309\n",
      "Epoch: 0141 dd_loss= 1.21948 gg_loss= 0.51255 cclass_loss= 1.13948 train_acc= 0.99167 test_acc= 0.70900 time= 865.80336\n",
      "Epoch: 0142 dd_loss= 1.15919 gg_loss= 0.55241 cclass_loss= 1.13020 train_acc= 0.99167 test_acc= 0.70800 time= 875.72193\n",
      "Epoch: 0143 dd_loss= 1.20283 gg_loss= 0.51895 cclass_loss= 1.14044 train_acc= 1.00000 test_acc= 0.70900 time= 885.65713\n",
      "Epoch: 0144 dd_loss= 1.21033 gg_loss= 0.51496 cclass_loss= 1.13904 train_acc= 1.00000 test_acc= 0.71500 time= 895.12109\n",
      "Epoch: 0145 dd_loss= 1.23345 gg_loss= 0.50144 cclass_loss= 1.12786 train_acc= 1.00000 test_acc= 0.71000 time= 903.29710\n",
      "Epoch: 0146 dd_loss= 1.16243 gg_loss= 0.55365 cclass_loss= 1.13608 train_acc= 1.00000 test_acc= 0.70700 time= 916.70202\n",
      "Epoch: 0147 dd_loss= 1.19231 gg_loss= 0.52626 cclass_loss= 1.13105 train_acc= 0.99167 test_acc= 0.70900 time= 926.43713\n",
      "Epoch: 0148 dd_loss= 1.19286 gg_loss= 0.52594 cclass_loss= 1.14085 train_acc= 0.99167 test_acc= 0.71100 time= 935.54105\n",
      "Epoch: 0149 dd_loss= 1.23038 gg_loss= 0.49715 cclass_loss= 1.14141 train_acc= 1.00000 test_acc= 0.71000 time= 945.36438\n",
      "Epoch: 0150 dd_loss= 1.15224 gg_loss= 0.55349 cclass_loss= 1.14804 train_acc= 0.98333 test_acc= 0.71300 time= 955.39470\n",
      "Epoch: 0151 dd_loss= 1.18378 gg_loss= 0.52799 cclass_loss= 1.13751 train_acc= 0.99167 test_acc= 0.71300 time= 964.97246\n",
      "Epoch: 0152 dd_loss= 1.18760 gg_loss= 0.52654 cclass_loss= 1.13868 train_acc= 0.99167 test_acc= 0.71000 time= 974.68997\n",
      "Epoch: 0153 dd_loss= 1.22101 gg_loss= 0.50418 cclass_loss= 1.13812 train_acc= 0.99167 test_acc= 0.70900 time= 983.60850\n",
      "Epoch: 0154 dd_loss= 1.14365 gg_loss= 0.55706 cclass_loss= 1.13539 train_acc= 1.00000 test_acc= 0.71300 time= 992.67499\n",
      "Epoch: 0155 dd_loss= 1.16229 gg_loss= 0.54344 cclass_loss= 1.13634 train_acc= 1.00000 test_acc= 0.71100 time= 998.43027\n",
      "Epoch: 0156 dd_loss= 1.16120 gg_loss= 0.54215 cclass_loss= 1.13987 train_acc= 0.98333 test_acc= 0.71400 time= 1004.73507\n",
      "Epoch: 0157 dd_loss= 1.21559 gg_loss= 0.50589 cclass_loss= 1.14984 train_acc= 0.98333 test_acc= 0.71200 time= 1009.54991\n",
      "Epoch: 0158 dd_loss= 1.17236 gg_loss= 0.53659 cclass_loss= 1.14154 train_acc= 0.99167 test_acc= 0.71100 time= 1014.58941\n",
      "Epoch: 0159 dd_loss= 1.16018 gg_loss= 0.54675 cclass_loss= 1.13808 train_acc= 0.98333 test_acc= 0.70900 time= 1019.07749\n",
      "Epoch: 0160 dd_loss= 1.16952 gg_loss= 0.53920 cclass_loss= 1.13543 train_acc= 0.99167 test_acc= 0.71100 time= 1023.59196\n",
      "Epoch: 0161 dd_loss= 1.20338 gg_loss= 0.51157 cclass_loss= 1.15169 train_acc= 0.99167 test_acc= 0.71100 time= 1030.03749\n",
      "Epoch: 0162 dd_loss= 1.16599 gg_loss= 0.53786 cclass_loss= 1.13276 train_acc= 0.99167 test_acc= 0.70700 time= 1036.33479\n",
      "Epoch: 0163 dd_loss= 1.13880 gg_loss= 0.55492 cclass_loss= 1.14636 train_acc= 0.99167 test_acc= 0.70700 time= 1043.31739\n",
      "Epoch: 0164 dd_loss= 1.15207 gg_loss= 0.54551 cclass_loss= 1.12734 train_acc= 0.99167 test_acc= 0.70900 time= 1050.28487\n",
      "Epoch: 0165 dd_loss= 1.19096 gg_loss= 0.51712 cclass_loss= 1.13734 train_acc= 0.99167 test_acc= 0.71000 time= 1057.27274\n",
      "Epoch: 0166 dd_loss= 1.17764 gg_loss= 0.52876 cclass_loss= 1.13204 train_acc= 1.00000 test_acc= 0.71100 time= 1064.21760\n",
      "Epoch: 0167 dd_loss= 1.11384 gg_loss= 0.57263 cclass_loss= 1.13374 train_acc= 1.00000 test_acc= 0.71300 time= 1073.78767\n",
      "Epoch: 0168 dd_loss= 1.14431 gg_loss= 0.54869 cclass_loss= 1.12990 train_acc= 0.99167 test_acc= 0.71600 time= 1081.35534\n",
      "Epoch: 0169 dd_loss= 1.17780 gg_loss= 0.52752 cclass_loss= 1.12784 train_acc= 0.99167 test_acc= 0.70600 time= 1088.11009\n",
      "Epoch: 0170 dd_loss= 1.16654 gg_loss= 0.53244 cclass_loss= 1.13745 train_acc= 1.00000 test_acc= 0.70800 time= 1094.78976\n",
      "Epoch: 0171 dd_loss= 1.11568 gg_loss= 0.57201 cclass_loss= 1.12954 train_acc= 1.00000 test_acc= 0.71200 time= 1102.60406\n",
      "Epoch: 0172 dd_loss= 1.14996 gg_loss= 0.54431 cclass_loss= 1.13687 train_acc= 1.00000 test_acc= 0.71600 time= 1109.67820\n",
      "Epoch: 0173 dd_loss= 1.16652 gg_loss= 0.53260 cclass_loss= 1.13335 train_acc= 1.00000 test_acc= 0.70600 time= 1116.64570\n",
      "Epoch: 0174 dd_loss= 1.18136 gg_loss= 0.52013 cclass_loss= 1.13772 train_acc= 1.00000 test_acc= 0.70500 time= 1124.76824\n",
      "Epoch: 0175 dd_loss= 1.09996 gg_loss= 0.57953 cclass_loss= 1.12856 train_acc= 0.99167 test_acc= 0.70700 time= 1134.53903\n",
      "Epoch: 0176 dd_loss= 1.12421 gg_loss= 0.55997 cclass_loss= 1.12750 train_acc= 1.00000 test_acc= 0.70900 time= 1149.73949\n",
      "Epoch: 0177 dd_loss= 1.15413 gg_loss= 0.53861 cclass_loss= 1.12871 train_acc= 0.99167 test_acc= 0.71400 time= 1161.82855\n",
      "Epoch: 0178 dd_loss= 1.18591 gg_loss= 0.51686 cclass_loss= 1.14080 train_acc= 1.00000 test_acc= 0.71200 time= 1172.64267\n",
      "Epoch: 0179 dd_loss= 1.10597 gg_loss= 0.57119 cclass_loss= 1.13585 train_acc= 1.00000 test_acc= 0.71000 time= 1182.29415\n",
      "Epoch: 0180 dd_loss= 1.14082 gg_loss= 0.54463 cclass_loss= 1.14049 train_acc= 0.98333 test_acc= 0.70900 time= 1191.47963\n",
      "Epoch: 0181 dd_loss= 1.13748 gg_loss= 0.54758 cclass_loss= 1.13316 train_acc= 1.00000 test_acc= 0.71500 time= 1201.42471\n",
      "Epoch: 0182 dd_loss= 1.18160 gg_loss= 0.51683 cclass_loss= 1.12787 train_acc= 1.00000 test_acc= 0.71100 time= 1213.45235\n",
      "Epoch: 0183 dd_loss= 1.11141 gg_loss= 0.56745 cclass_loss= 1.13197 train_acc= 1.00000 test_acc= 0.71200 time= 1220.17190\n",
      "Epoch: 0184 dd_loss= 1.11266 gg_loss= 0.56425 cclass_loss= 1.13541 train_acc= 1.00000 test_acc= 0.70900 time= 1226.85761\n",
      "Epoch: 0185 dd_loss= 1.13244 gg_loss= 0.55138 cclass_loss= 1.13270 train_acc= 1.00000 test_acc= 0.71100 time= 1233.76647\n",
      "Epoch: 0186 dd_loss= 1.17584 gg_loss= 0.52181 cclass_loss= 1.13121 train_acc= 0.99167 test_acc= 0.70900 time= 1240.44855\n",
      "Epoch: 0187 dd_loss= 1.11092 gg_loss= 0.56441 cclass_loss= 1.13203 train_acc= 0.99167 test_acc= 0.71000 time= 1247.90604\n",
      "Epoch: 0188 dd_loss= 1.11725 gg_loss= 0.56328 cclass_loss= 1.14262 train_acc= 0.98333 test_acc= 0.71200 time= 1254.96374\n",
      "Epoch: 0189 dd_loss= 1.11736 gg_loss= 0.55733 cclass_loss= 1.13638 train_acc= 0.99167 test_acc= 0.71000 time= 1261.90589\n",
      "Epoch: 0190 dd_loss= 1.17853 gg_loss= 0.52107 cclass_loss= 1.13639 train_acc= 0.99167 test_acc= 0.70900 time= 1269.42802\n",
      "Epoch: 0191 dd_loss= 1.11863 gg_loss= 0.56152 cclass_loss= 1.13549 train_acc= 1.00000 test_acc= 0.71600 time= 1278.59613\n",
      "Epoch: 0192 dd_loss= 1.09208 gg_loss= 0.57778 cclass_loss= 1.13479 train_acc= 1.00000 test_acc= 0.71200 time= 1288.24798\n",
      "Epoch: 0193 dd_loss= 1.11615 gg_loss= 0.56166 cclass_loss= 1.13003 train_acc= 1.00000 test_acc= 0.71000 time= 1298.19383\n",
      "Epoch: 0194 dd_loss= 1.15582 gg_loss= 0.53416 cclass_loss= 1.12861 train_acc= 0.99167 test_acc= 0.70900 time= 1307.93416\n",
      "Epoch: 0195 dd_loss= 1.14136 gg_loss= 0.54397 cclass_loss= 1.13914 train_acc= 1.00000 test_acc= 0.71700 time= 1315.94175\n",
      "Epoch: 0196 dd_loss= 1.07515 gg_loss= 0.59076 cclass_loss= 1.12469 train_acc= 0.99167 test_acc= 0.71600 time= 1325.42914\n",
      "Epoch: 0197 dd_loss= 1.12027 gg_loss= 0.55226 cclass_loss= 1.12667 train_acc= 0.99167 test_acc= 0.71200 time= 1334.98019\n",
      "Epoch: 0198 dd_loss= 1.14272 gg_loss= 0.53907 cclass_loss= 1.13700 train_acc= 0.99167 test_acc= 0.70900 time= 1342.19113\n",
      "Epoch: 0199 dd_loss= 1.16202 gg_loss= 0.52841 cclass_loss= 1.13011 train_acc= 0.99167 test_acc= 0.71800 time= 1349.11254\n",
      "Epoch: 0200 dd_loss= 1.08034 gg_loss= 0.58303 cclass_loss= 1.12585 train_acc= 1.00000 test_acc= 0.71500 time= 1355.81207\n",
      "Epoch: 0201 dd_loss= 1.12669 gg_loss= 0.55029 cclass_loss= 1.12794 train_acc= 1.00000 test_acc= 0.71000 time= 1362.64155\n",
      "Epoch: 0202 dd_loss= 1.13232 gg_loss= 0.54425 cclass_loss= 1.13839 train_acc= 0.97500 test_acc= 0.71300 time= 1369.79986\n",
      "Epoch: 0203 dd_loss= 1.16081 gg_loss= 0.52718 cclass_loss= 1.13048 train_acc= 0.99167 test_acc= 0.71700 time= 1378.80746\n",
      "Epoch: 0204 dd_loss= 1.08075 gg_loss= 0.58103 cclass_loss= 1.13076 train_acc= 0.99167 test_acc= 0.71800 time= 1387.41103\n",
      "Epoch: 0205 dd_loss= 1.11227 gg_loss= 0.55713 cclass_loss= 1.13192 train_acc= 0.99167 test_acc= 0.71200 time= 1397.26345\n",
      "Epoch: 0206 dd_loss= 1.12308 gg_loss= 0.55109 cclass_loss= 1.12733 train_acc= 1.00000 test_acc= 0.71500 time= 1407.54831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0207 dd_loss= 1.16380 gg_loss= 0.52419 cclass_loss= 1.13105 train_acc= 1.00000 test_acc= 0.71600 time= 1416.21944\n",
      "Epoch: 0208 dd_loss= 1.06704 gg_loss= 0.58979 cclass_loss= 1.13152 train_acc= 0.99167 test_acc= 0.71500 time= 1424.13856\n",
      "Epoch: 0209 dd_loss= 1.13085 gg_loss= 0.55298 cclass_loss= 1.13546 train_acc= 0.99167 test_acc= 0.71300 time= 1432.78804\n",
      "Epoch: 0210 dd_loss= 1.12629 gg_loss= 0.55301 cclass_loss= 1.12626 train_acc= 0.99167 test_acc= 0.71100 time= 1445.06141\n",
      "Epoch: 0211 dd_loss= 1.16992 gg_loss= 0.52625 cclass_loss= 1.13038 train_acc= 1.00000 test_acc= 0.71500 time= 1454.82662\n",
      "Epoch: 0212 dd_loss= 1.09207 gg_loss= 0.57375 cclass_loss= 1.13004 train_acc= 0.99167 test_acc= 0.71400 time= 1463.25834\n",
      "Epoch: 0213 dd_loss= 1.09225 gg_loss= 0.57311 cclass_loss= 1.13932 train_acc= 0.99167 test_acc= 0.71400 time= 1470.53698\n",
      "Epoch: 0214 dd_loss= 1.12300 gg_loss= 0.55369 cclass_loss= 1.13537 train_acc= 0.99167 test_acc= 0.71500 time= 1477.08079\n",
      "Epoch: 0215 dd_loss= 1.15873 gg_loss= 0.52849 cclass_loss= 1.13365 train_acc= 0.98333 test_acc= 0.71200 time= 1483.84496\n",
      "Epoch: 0216 dd_loss= 1.09213 gg_loss= 0.56945 cclass_loss= 1.12336 train_acc= 0.99167 test_acc= 0.71300 time= 1491.01508\n",
      "Epoch: 0217 dd_loss= 1.08324 gg_loss= 0.57682 cclass_loss= 1.13160 train_acc= 1.00000 test_acc= 0.71400 time= 1498.53421\n",
      "Epoch: 0218 dd_loss= 1.10100 gg_loss= 0.56193 cclass_loss= 1.13056 train_acc= 1.00000 test_acc= 0.71600 time= 1508.39020\n",
      "Epoch: 0219 dd_loss= 1.15305 gg_loss= 0.52598 cclass_loss= 1.13517 train_acc= 0.99167 test_acc= 0.71800 time= 1516.69565\n",
      "Epoch: 0220 dd_loss= 1.11270 gg_loss= 0.55221 cclass_loss= 1.14460 train_acc= 0.99167 test_acc= 0.71800 time= 1524.46838\n",
      "Epoch: 0221 dd_loss= 1.07789 gg_loss= 0.57582 cclass_loss= 1.13695 train_acc= 1.00000 test_acc= 0.71000 time= 1531.81913\n",
      "Epoch: 0222 dd_loss= 1.08789 gg_loss= 0.57272 cclass_loss= 1.12942 train_acc= 0.99167 test_acc= 0.71400 time= 1538.38308\n",
      "Epoch: 0223 dd_loss= 1.11769 gg_loss= 0.54891 cclass_loss= 1.13697 train_acc= 0.99167 test_acc= 0.72100 time= 1544.98420\n",
      "Epoch: 0224 dd_loss= 1.10312 gg_loss= 0.55917 cclass_loss= 1.14302 train_acc= 0.98333 test_acc= 0.71900 time= 1552.20869\n",
      "Epoch: 0225 dd_loss= 1.06536 gg_loss= 0.58706 cclass_loss= 1.13240 train_acc= 0.99167 test_acc= 0.71500 time= 1560.16651\n",
      "Epoch: 0226 dd_loss= 1.06817 gg_loss= 0.58226 cclass_loss= 1.13918 train_acc= 0.99167 test_acc= 0.71700 time= 1568.74575\n",
      "Epoch: 0227 dd_loss= 1.09795 gg_loss= 0.56116 cclass_loss= 1.12985 train_acc= 1.00000 test_acc= 0.71800 time= 1576.22869\n",
      "Epoch: 0228 dd_loss= 1.11160 gg_loss= 0.55240 cclass_loss= 1.14249 train_acc= 0.99167 test_acc= 0.71800 time= 1582.99405\n",
      "Epoch: 0229 dd_loss= 1.03417 gg_loss= 0.60876 cclass_loss= 1.13095 train_acc= 1.00000 test_acc= 0.71800 time= 1591.61523\n",
      "Epoch: 0230 dd_loss= 1.08194 gg_loss= 0.57369 cclass_loss= 1.13831 train_acc= 0.99167 test_acc= 0.71600 time= 1600.81185\n",
      "Epoch: 0231 dd_loss= 1.09037 gg_loss= 0.56421 cclass_loss= 1.13422 train_acc= 0.99167 test_acc= 0.71700 time= 1609.58634\n",
      "Epoch: 0232 dd_loss= 1.09965 gg_loss= 0.55589 cclass_loss= 1.12197 train_acc= 1.00000 test_acc= 0.71800 time= 1616.27928\n",
      "Epoch: 0233 dd_loss= 1.02275 gg_loss= 0.61613 cclass_loss= 1.13528 train_acc= 0.99167 test_acc= 0.71900 time= 1622.29601\n",
      "Epoch: 0234 dd_loss= 1.07443 gg_loss= 0.57798 cclass_loss= 1.12915 train_acc= 1.00000 test_acc= 0.72300 time= 1629.10058\n",
      "Epoch: 0235 dd_loss= 1.08783 gg_loss= 0.56910 cclass_loss= 1.12770 train_acc= 0.99167 test_acc= 0.71900 time= 1636.05506\n",
      "Epoch: 0236 dd_loss= 1.13136 gg_loss= 0.53712 cclass_loss= 1.13427 train_acc= 0.99167 test_acc= 0.71700 time= 1646.59060\n",
      "Epoch: 0237 dd_loss= 1.03679 gg_loss= 0.60248 cclass_loss= 1.12637 train_acc= 1.00000 test_acc= 0.71500 time= 1654.08818\n",
      "Epoch: 0238 dd_loss= 1.06518 gg_loss= 0.58146 cclass_loss= 1.12679 train_acc= 1.00000 test_acc= 0.72100 time= 1660.94628\n",
      "Epoch: 0239 dd_loss= 1.09638 gg_loss= 0.56339 cclass_loss= 1.13024 train_acc= 0.99167 test_acc= 0.72000 time= 1668.05705\n",
      "Epoch: 0240 dd_loss= 1.10898 gg_loss= 0.54725 cclass_loss= 1.12865 train_acc= 0.99167 test_acc= 0.71800 time= 1674.79910\n",
      "Epoch: 0241 dd_loss= 1.04446 gg_loss= 0.59960 cclass_loss= 1.12749 train_acc= 0.99167 test_acc= 0.71600 time= 1682.50587\n",
      "Epoch: 0242 dd_loss= 1.05163 gg_loss= 0.58794 cclass_loss= 1.12601 train_acc= 0.99167 test_acc= 0.71900 time= 1689.46228\n",
      "Epoch: 0243 dd_loss= 1.04933 gg_loss= 0.59160 cclass_loss= 1.12534 train_acc= 0.99167 test_acc= 0.71900 time= 1695.76834\n",
      "Epoch: 0244 dd_loss= 1.12239 gg_loss= 0.54031 cclass_loss= 1.12244 train_acc= 0.99167 test_acc= 0.71900 time= 1702.86958\n",
      "Epoch: 0245 dd_loss= 1.06783 gg_loss= 0.58052 cclass_loss= 1.12836 train_acc= 1.00000 test_acc= 0.71600 time= 1710.02693\n",
      "Epoch: 0246 dd_loss= 1.04606 gg_loss= 0.59812 cclass_loss= 1.12945 train_acc= 1.00000 test_acc= 0.71800 time= 1717.07390\n",
      "Epoch: 0247 dd_loss= 1.04973 gg_loss= 0.59212 cclass_loss= 1.12929 train_acc= 1.00000 test_acc= 0.71700 time= 1724.15148\n",
      "Epoch: 0248 dd_loss= 1.11948 gg_loss= 0.54288 cclass_loss= 1.13919 train_acc= 1.00000 test_acc= 0.71900 time= 1730.69784\n",
      "Epoch: 0249 dd_loss= 1.05733 gg_loss= 0.58360 cclass_loss= 1.12507 train_acc= 0.99167 test_acc= 0.72100 time= 1740.25294\n",
      "Epoch: 0250 dd_loss= 1.04659 gg_loss= 0.58983 cclass_loss= 1.12558 train_acc= 0.99167 test_acc= 0.72100 time= 1747.51699\n",
      "Epoch: 0251 dd_loss= 1.06629 gg_loss= 0.57849 cclass_loss= 1.13489 train_acc= 0.98333 test_acc= 0.72000 time= 1754.67351\n",
      "Epoch: 0252 dd_loss= 1.11140 gg_loss= 0.54556 cclass_loss= 1.12574 train_acc= 1.00000 test_acc= 0.71700 time= 1761.47915\n",
      "Epoch: 0253 dd_loss= 1.07818 gg_loss= 0.56822 cclass_loss= 1.13167 train_acc= 0.99167 test_acc= 0.72100 time= 1768.26687\n",
      "Epoch: 0254 dd_loss= 1.04548 gg_loss= 0.59214 cclass_loss= 1.13205 train_acc= 0.99167 test_acc= 0.72000 time= 1775.22286\n",
      "Epoch: 0255 dd_loss= 1.04488 gg_loss= 0.58999 cclass_loss= 1.12514 train_acc= 0.99167 test_acc= 0.72000 time= 1784.27776\n",
      "Epoch: 0256 dd_loss= 1.09587 gg_loss= 0.55933 cclass_loss= 1.13246 train_acc= 1.00000 test_acc= 0.71500 time= 1790.87140\n",
      "Epoch: 0257 dd_loss= 1.09967 gg_loss= 0.55560 cclass_loss= 1.12760 train_acc= 0.99167 test_acc= 0.71800 time= 1797.17303\n",
      "Epoch: 0258 dd_loss= 1.01888 gg_loss= 0.61452 cclass_loss= 1.13362 train_acc= 0.99167 test_acc= 0.72100 time= 1803.53263\n",
      "Epoch: 0259 dd_loss= 1.04666 gg_loss= 0.59188 cclass_loss= 1.12134 train_acc= 1.00000 test_acc= 0.72300 time= 1810.85096\n",
      "Epoch: 0260 dd_loss= 1.07033 gg_loss= 0.57757 cclass_loss= 1.12461 train_acc= 0.99167 test_acc= 0.71800 time= 1817.43307\n",
      "Epoch: 0261 dd_loss= 1.09205 gg_loss= 0.55894 cclass_loss= 1.12871 train_acc= 1.00000 test_acc= 0.71900 time= 1824.79049\n",
      "Epoch: 0262 dd_loss= 1.02268 gg_loss= 0.60969 cclass_loss= 1.13579 train_acc= 1.00000 test_acc= 0.72100 time= 1831.17026\n",
      "Epoch: 0263 dd_loss= 1.03786 gg_loss= 0.59529 cclass_loss= 1.12680 train_acc= 0.99167 test_acc= 0.72100 time= 1836.62884\n",
      "Epoch: 0264 dd_loss= 1.06163 gg_loss= 0.58087 cclass_loss= 1.12517 train_acc= 1.00000 test_acc= 0.72300 time= 1843.20456\n",
      "Epoch: 0265 dd_loss= 1.11625 gg_loss= 0.54156 cclass_loss= 1.13262 train_acc= 1.00000 test_acc= 0.72200 time= 1850.29519\n",
      "Epoch: 0266 dd_loss= 1.01344 gg_loss= 0.61529 cclass_loss= 1.12699 train_acc= 0.99167 test_acc= 0.72000 time= 1856.65043\n",
      "Epoch: 0267 dd_loss= 1.02685 gg_loss= 0.60127 cclass_loss= 1.12738 train_acc= 0.99167 test_acc= 0.72200 time= 1863.24745\n",
      "Epoch: 0268 dd_loss= 1.04383 gg_loss= 0.59197 cclass_loss= 1.13639 train_acc= 1.00000 test_acc= 0.72000 time= 1871.13529\n",
      "Epoch: 0269 dd_loss= 1.09692 gg_loss= 0.55382 cclass_loss= 1.12436 train_acc= 0.99167 test_acc= 0.72300 time= 1880.40157\n",
      "Epoch: 0270 dd_loss= 1.01254 gg_loss= 0.61318 cclass_loss= 1.13013 train_acc= 1.00000 test_acc= 0.72300 time= 1889.75765\n",
      "Epoch: 0271 dd_loss= 1.02594 gg_loss= 0.60099 cclass_loss= 1.13363 train_acc= 0.99167 test_acc= 0.71600 time= 1897.89601\n",
      "Epoch: 0272 dd_loss= 1.03730 gg_loss= 0.59574 cclass_loss= 1.12413 train_acc= 1.00000 test_acc= 0.71800 time= 1904.41687\n",
      "Epoch: 0273 dd_loss= 1.09393 gg_loss= 0.55126 cclass_loss= 1.12547 train_acc= 0.99167 test_acc= 0.72400 time= 1911.44366\n",
      "Epoch: 0274 dd_loss= 1.04524 gg_loss= 0.58620 cclass_loss= 1.13096 train_acc= 0.98333 test_acc= 0.72500 time= 1918.40112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0275 dd_loss= 1.01682 gg_loss= 0.60707 cclass_loss= 1.12527 train_acc= 0.99167 test_acc= 0.72000 time= 1925.21103\n",
      "Epoch: 0276 dd_loss= 1.03591 gg_loss= 0.58914 cclass_loss= 1.12283 train_acc= 1.00000 test_acc= 0.72100 time= 1931.99525\n",
      "Epoch: 0277 dd_loss= 1.08689 gg_loss= 0.55915 cclass_loss= 1.12646 train_acc= 0.99167 test_acc= 0.72100 time= 1938.67093\n",
      "Epoch: 0278 dd_loss= 1.03075 gg_loss= 0.59406 cclass_loss= 1.13929 train_acc= 0.99167 test_acc= 0.72200 time= 1945.05637\n",
      "Epoch: 0279 dd_loss= 1.01863 gg_loss= 0.60419 cclass_loss= 1.12234 train_acc= 0.99167 test_acc= 0.72300 time= 1951.67533\n",
      "Epoch: 0280 dd_loss= 1.01959 gg_loss= 0.60311 cclass_loss= 1.13085 train_acc= 0.99167 test_acc= 0.72100 time= 1958.86930\n",
      "Epoch: 0281 dd_loss= 1.08053 gg_loss= 0.56480 cclass_loss= 1.13007 train_acc= 1.00000 test_acc= 0.71900 time= 1965.52296\n",
      "Epoch: 0282 dd_loss= 1.06391 gg_loss= 0.57128 cclass_loss= 1.13601 train_acc= 1.00000 test_acc= 0.72100 time= 1972.79617\n",
      "Epoch: 0283 dd_loss= 1.00831 gg_loss= 0.61150 cclass_loss= 1.12720 train_acc= 0.99167 test_acc= 0.72100 time= 1979.70983\n",
      "Epoch: 0284 dd_loss= 1.01706 gg_loss= 0.60326 cclass_loss= 1.12923 train_acc= 0.99167 test_acc= 0.72100 time= 1985.98665\n",
      "Epoch: 0285 dd_loss= 1.06853 gg_loss= 0.57020 cclass_loss= 1.12203 train_acc= 1.00000 test_acc= 0.71600 time= 1993.04646\n",
      "Epoch: 0286 dd_loss= 1.09241 gg_loss= 0.55169 cclass_loss= 1.12467 train_acc= 0.99167 test_acc= 0.72100 time= 2000.02965\n",
      "Epoch: 0287 dd_loss= 0.98472 gg_loss= 0.63061 cclass_loss= 1.12553 train_acc= 1.00000 test_acc= 0.72400 time= 2007.32811\n",
      "Epoch: 0288 dd_loss= 1.01395 gg_loss= 0.60594 cclass_loss= 1.12270 train_acc= 0.99167 test_acc= 0.72300 time= 2014.81881\n",
      "Epoch: 0289 dd_loss= 1.03579 gg_loss= 0.59151 cclass_loss= 1.12772 train_acc= 0.98333 test_acc= 0.72000 time= 2022.04390\n",
      "Epoch: 0290 dd_loss= 1.07016 gg_loss= 0.56511 cclass_loss= 1.12367 train_acc= 0.99167 test_acc= 0.72100 time= 2028.92740\n",
      "Epoch: 0291 dd_loss= 0.98364 gg_loss= 0.62838 cclass_loss= 1.12623 train_acc= 0.99167 test_acc= 0.72200 time= 2036.46094\n",
      "Epoch: 0292 dd_loss= 1.00398 gg_loss= 0.61012 cclass_loss= 1.13227 train_acc= 0.99167 test_acc= 0.72200 time= 2042.97963\n",
      "Epoch: 0293 dd_loss= 1.01754 gg_loss= 0.60254 cclass_loss= 1.12848 train_acc= 0.99167 test_acc= 0.71900 time= 2049.28914\n",
      "Epoch: 0294 dd_loss= 1.05366 gg_loss= 0.57774 cclass_loss= 1.11892 train_acc= 1.00000 test_acc= 0.71900 time= 2056.36300\n",
      "Epoch: 0295 dd_loss= 1.00733 gg_loss= 0.60506 cclass_loss= 1.12579 train_acc= 0.99167 test_acc= 0.72100 time= 2063.83902\n",
      "Epoch: 0296 dd_loss= 1.00254 gg_loss= 0.61598 cclass_loss= 1.12930 train_acc= 0.99167 test_acc= 0.71800 time= 2070.78683\n",
      "Epoch: 0297 dd_loss= 1.02147 gg_loss= 0.59823 cclass_loss= 1.12879 train_acc= 0.99167 test_acc= 0.71900 time= 2078.94665\n",
      "Epoch: 0298 dd_loss= 1.06581 gg_loss= 0.56844 cclass_loss= 1.13228 train_acc= 0.99167 test_acc= 0.71900 time= 2088.88599\n",
      "Epoch: 0299 dd_loss= 1.00828 gg_loss= 0.60699 cclass_loss= 1.13695 train_acc= 0.99167 test_acc= 0.72400 time= 2097.92152\n",
      "Epoch: 0300 dd_loss= 1.00965 gg_loss= 0.60437 cclass_loss= 1.13261 train_acc= 0.99167 test_acc= 0.72500 time= 2115.02413\n",
      "Epoch: 0301 dd_loss= 1.00185 gg_loss= 0.61055 cclass_loss= 1.12542 train_acc= 0.99167 test_acc= 0.72100 time= 2129.20663\n",
      "Epoch: 0302 dd_loss= 1.07086 gg_loss= 0.56423 cclass_loss= 1.14016 train_acc= 0.99167 test_acc= 0.71800 time= 2138.66553\n",
      "Epoch: 0303 dd_loss= 1.00057 gg_loss= 0.61280 cclass_loss= 1.12594 train_acc= 1.00000 test_acc= 0.72000 time= 2148.71328\n",
      "Epoch: 0304 dd_loss= 0.98959 gg_loss= 0.61796 cclass_loss= 1.13800 train_acc= 0.99167 test_acc= 0.72200 time= 2156.83496\n",
      "Epoch: 0305 dd_loss= 1.00427 gg_loss= 0.61211 cclass_loss= 1.12650 train_acc= 0.98333 test_acc= 0.72400 time= 2164.03364\n",
      "Epoch: 0306 dd_loss= 1.08718 gg_loss= 0.55243 cclass_loss= 1.12458 train_acc= 0.99167 test_acc= 0.72200 time= 2169.76363\n",
      "Epoch: 0307 dd_loss= 1.02757 gg_loss= 0.59490 cclass_loss= 1.12595 train_acc= 1.00000 test_acc= 0.72100 time= 2174.52628\n",
      "Epoch: 0308 dd_loss= 0.98548 gg_loss= 0.62078 cclass_loss= 1.13047 train_acc= 0.99167 test_acc= 0.72300 time= 2178.98327\n",
      "Epoch: 0309 dd_loss= 1.01043 gg_loss= 0.60841 cclass_loss= 1.11818 train_acc= 1.00000 test_acc= 0.72500 time= 2183.41467\n",
      "Epoch: 0310 dd_loss= 1.07034 gg_loss= 0.55874 cclass_loss= 1.12442 train_acc= 0.99167 test_acc= 0.72300 time= 2188.07199\n",
      "Epoch: 0311 dd_loss= 1.02030 gg_loss= 0.59616 cclass_loss= 1.12643 train_acc= 0.99167 test_acc= 0.71800 time= 2194.69961\n",
      "Epoch: 0312 dd_loss= 0.96201 gg_loss= 0.64206 cclass_loss= 1.12407 train_acc= 0.99167 test_acc= 0.71900 time= 2201.39861\n",
      "Epoch: 0313 dd_loss= 1.01158 gg_loss= 0.60685 cclass_loss= 1.12180 train_acc= 1.00000 test_acc= 0.72200 time= 2208.04687\n",
      "Epoch: 0314 dd_loss= 1.07593 gg_loss= 0.56248 cclass_loss= 1.12641 train_acc= 0.99167 test_acc= 0.72400 time= 2215.50039\n",
      "Epoch: 0315 dd_loss= 1.04320 gg_loss= 0.58200 cclass_loss= 1.13030 train_acc= 0.99167 test_acc= 0.72100 time= 2222.52705\n",
      "Epoch: 0316 dd_loss= 0.98012 gg_loss= 0.62240 cclass_loss= 1.12368 train_acc= 1.00000 test_acc= 0.72400 time= 2229.54347\n",
      "Epoch: 0317 dd_loss= 1.00694 gg_loss= 0.60688 cclass_loss= 1.12293 train_acc= 0.99167 test_acc= 0.72500 time= 2235.86728\n",
      "Epoch: 0318 dd_loss= 1.03302 gg_loss= 0.58400 cclass_loss= 1.12996 train_acc= 1.00000 test_acc= 0.72200 time= 2243.19642\n",
      "Epoch: 0319 dd_loss= 1.04067 gg_loss= 0.57911 cclass_loss= 1.12129 train_acc= 1.00000 test_acc= 0.72200 time= 2250.26873\n",
      "Epoch: 0320 dd_loss= 0.97807 gg_loss= 0.62820 cclass_loss= 1.12689 train_acc= 1.00000 test_acc= 0.72100 time= 2257.26622\n",
      "Epoch: 0321 dd_loss= 0.99456 gg_loss= 0.61394 cclass_loss= 1.12683 train_acc= 0.99167 test_acc= 0.72100 time= 2264.28200\n",
      "Epoch: 0322 dd_loss= 1.01927 gg_loss= 0.59415 cclass_loss= 1.13258 train_acc= 1.00000 test_acc= 0.72000 time= 2270.67087\n",
      "Epoch: 0323 dd_loss= 1.06671 gg_loss= 0.56563 cclass_loss= 1.13222 train_acc= 0.99167 test_acc= 0.71900 time= 2277.32627\n",
      "Epoch: 0324 dd_loss= 0.97557 gg_loss= 0.62751 cclass_loss= 1.12691 train_acc= 0.99167 test_acc= 0.72300 time= 2284.09811\n",
      "Epoch: 0325 dd_loss= 1.00626 gg_loss= 0.60868 cclass_loss= 1.11820 train_acc= 1.00000 test_acc= 0.72300 time= 2290.77666\n",
      "Epoch: 0326 dd_loss= 1.01293 gg_loss= 0.60096 cclass_loss= 1.13183 train_acc= 0.98333 test_acc= 0.72100 time= 2297.52132\n",
      "Epoch: 0327 dd_loss= 1.05625 gg_loss= 0.56639 cclass_loss= 1.13236 train_acc= 0.99167 test_acc= 0.72100 time= 2304.42404\n",
      "Epoch: 0328 dd_loss= 0.97426 gg_loss= 0.62759 cclass_loss= 1.13110 train_acc= 1.00000 test_acc= 0.72000 time= 2310.92836\n",
      "Epoch: 0329 dd_loss= 0.99186 gg_loss= 0.61599 cclass_loss= 1.13426 train_acc= 0.99167 test_acc= 0.71600 time= 2317.39026\n",
      "Epoch: 0330 dd_loss= 1.00757 gg_loss= 0.60615 cclass_loss= 1.12020 train_acc= 1.00000 test_acc= 0.71700 time= 2323.72017\n",
      "Epoch: 0331 dd_loss= 1.07138 gg_loss= 0.56007 cclass_loss= 1.13179 train_acc= 0.98333 test_acc= 0.72100 time= 2330.73526\n",
      "Epoch: 0332 dd_loss= 1.02747 gg_loss= 0.59298 cclass_loss= 1.12604 train_acc= 1.00000 test_acc= 0.71900 time= 2338.35873\n",
      "Epoch: 0333 dd_loss= 0.98191 gg_loss= 0.62048 cclass_loss= 1.12205 train_acc= 1.00000 test_acc= 0.72100 time= 2345.92111\n",
      "Epoch: 0334 dd_loss= 1.00144 gg_loss= 0.61016 cclass_loss= 1.12164 train_acc= 0.99167 test_acc= 0.72200 time= 2352.41478\n",
      "Epoch: 0335 dd_loss= 1.07267 gg_loss= 0.55665 cclass_loss= 1.11822 train_acc= 0.99167 test_acc= 0.72000 time= 2359.19791\n",
      "Epoch: 0336 dd_loss= 1.00232 gg_loss= 0.60707 cclass_loss= 1.12399 train_acc= 1.00000 test_acc= 0.72300 time= 2366.99439\n",
      "Epoch: 0337 dd_loss= 0.97431 gg_loss= 0.62444 cclass_loss= 1.12279 train_acc= 1.00000 test_acc= 0.72200 time= 2380.19810\n",
      "Epoch: 0338 dd_loss= 0.99522 gg_loss= 0.61251 cclass_loss= 1.11774 train_acc= 0.99167 test_acc= 0.71900 time= 2387.85514\n",
      "Epoch: 0339 dd_loss= 1.05950 gg_loss= 0.57060 cclass_loss= 1.12253 train_acc= 0.99167 test_acc= 0.72100 time= 2394.78593\n",
      "Epoch: 0340 dd_loss= 1.02433 gg_loss= 0.59214 cclass_loss= 1.12808 train_acc= 0.99167 test_acc= 0.71500 time= 2401.68624\n",
      "Epoch: 0341 dd_loss= 0.97330 gg_loss= 0.62681 cclass_loss= 1.12680 train_acc= 0.99167 test_acc= 0.71600 time= 2408.46141\n",
      "Epoch: 0342 dd_loss= 0.99278 gg_loss= 0.61716 cclass_loss= 1.13709 train_acc= 0.99167 test_acc= 0.72000 time= 2415.08455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0343 dd_loss= 1.03640 gg_loss= 0.58445 cclass_loss= 1.12731 train_acc= 1.00000 test_acc= 0.72400 time= 2421.59794\n",
      "Epoch: 0344 dd_loss= 1.02070 gg_loss= 0.59388 cclass_loss= 1.12784 train_acc= 0.99167 test_acc= 0.71800 time= 2429.14601\n",
      "Epoch: 0345 dd_loss= 0.96874 gg_loss= 0.63626 cclass_loss= 1.13432 train_acc= 0.99167 test_acc= 0.71300 time= 2436.18298\n",
      "Epoch: 0346 dd_loss= 0.99221 gg_loss= 0.61216 cclass_loss= 1.13346 train_acc= 1.00000 test_acc= 0.71700 time= 2442.98969\n",
      "Epoch: 0347 dd_loss= 1.00567 gg_loss= 0.60541 cclass_loss= 1.12476 train_acc= 1.00000 test_acc= 0.72400 time= 2451.26430\n",
      "Epoch: 0348 dd_loss= 1.02093 gg_loss= 0.59317 cclass_loss= 1.11702 train_acc= 0.99167 test_acc= 0.72200 time= 2461.33910\n",
      "Epoch: 0349 dd_loss= 0.95130 gg_loss= 0.64684 cclass_loss= 1.13432 train_acc= 0.99167 test_acc= 0.72000 time= 2470.22738\n",
      "Epoch: 0350 dd_loss= 0.99600 gg_loss= 0.61342 cclass_loss= 1.13516 train_acc= 0.99167 test_acc= 0.72000 time= 2476.61410\n",
      "Epoch: 0351 dd_loss= 1.00654 gg_loss= 0.60029 cclass_loss= 1.13049 train_acc= 0.99167 test_acc= 0.72200 time= 2483.27008\n",
      "Epoch: 0352 dd_loss= 1.06246 gg_loss= 0.56909 cclass_loss= 1.12342 train_acc= 1.00000 test_acc= 0.71700 time= 2490.01285\n",
      "Epoch: 0353 dd_loss= 0.98670 gg_loss= 0.61705 cclass_loss= 1.12573 train_acc= 0.99167 test_acc= 0.71900 time= 2497.40792\n",
      "Epoch: 0354 dd_loss= 0.99539 gg_loss= 0.61259 cclass_loss= 1.12547 train_acc= 0.99167 test_acc= 0.72100 time= 2506.55961\n",
      "Epoch: 0355 dd_loss= 1.03628 gg_loss= 0.58886 cclass_loss= 1.14520 train_acc= 0.98333 test_acc= 0.72300 time= 2515.37621\n",
      "Epoch: 0356 dd_loss= 1.08364 gg_loss= 0.55618 cclass_loss= 1.12333 train_acc= 1.00000 test_acc= 0.71800 time= 2525.72665\n",
      "Epoch: 0357 dd_loss= 0.99594 gg_loss= 0.61222 cclass_loss= 1.12227 train_acc= 0.99167 test_acc= 0.71800 time= 2532.17925\n",
      "Epoch: 0358 dd_loss= 0.99242 gg_loss= 0.61889 cclass_loss= 1.13922 train_acc= 0.98333 test_acc= 0.71300 time= 2538.26639\n",
      "Epoch: 0359 dd_loss= 1.00047 gg_loss= 0.60774 cclass_loss= 1.12950 train_acc= 0.99167 test_acc= 0.71800 time= 2544.35160\n",
      "Epoch: 0360 dd_loss= 1.05949 gg_loss= 0.55962 cclass_loss= 1.12334 train_acc= 1.00000 test_acc= 0.71700 time= 2550.33339\n",
      "Epoch: 0361 dd_loss= 1.00073 gg_loss= 0.60565 cclass_loss= 1.13396 train_acc= 1.00000 test_acc= 0.72000 time= 2556.44167\n",
      "Epoch: 0362 dd_loss= 0.99373 gg_loss= 0.61378 cclass_loss= 1.12636 train_acc= 0.99167 test_acc= 0.71500 time= 2562.26248\n",
      "Epoch: 0363 dd_loss= 0.98566 gg_loss= 0.61553 cclass_loss= 1.11960 train_acc= 1.00000 test_acc= 0.71700 time= 2568.54778\n",
      "Epoch: 0364 dd_loss= 1.05517 gg_loss= 0.56669 cclass_loss= 1.13003 train_acc= 0.99167 test_acc= 0.71900 time= 2575.27408\n",
      "Epoch: 0365 dd_loss= 1.00457 gg_loss= 0.60303 cclass_loss= 1.12729 train_acc= 0.99167 test_acc= 0.71400 time= 2581.72353\n",
      "Epoch: 0366 dd_loss= 0.97106 gg_loss= 0.62454 cclass_loss= 1.12922 train_acc= 1.00000 test_acc= 0.71200 time= 2588.30942\n",
      "Epoch: 0367 dd_loss= 0.96870 gg_loss= 0.62964 cclass_loss= 1.12498 train_acc= 0.99167 test_acc= 0.72000 time= 2594.40938\n",
      "Epoch: 0368 dd_loss= 1.04280 gg_loss= 0.57398 cclass_loss= 1.13068 train_acc= 0.99167 test_acc= 0.72200 time= 2600.54007\n",
      "Epoch: 0369 dd_loss= 0.99236 gg_loss= 0.60881 cclass_loss= 1.12552 train_acc= 0.99167 test_acc= 0.72200 time= 2606.64786\n",
      "Epoch: 0370 dd_loss= 0.95179 gg_loss= 0.63609 cclass_loss= 1.13011 train_acc= 0.99167 test_acc= 0.71900 time= 2613.25015\n",
      "Epoch: 0371 dd_loss= 0.96901 gg_loss= 0.62850 cclass_loss= 1.12582 train_acc= 0.99167 test_acc= 0.71500 time= 2619.46250\n",
      "Epoch: 0372 dd_loss= 1.03235 gg_loss= 0.58054 cclass_loss= 1.12237 train_acc= 0.99167 test_acc= 0.72000 time= 2625.37678\n",
      "Epoch: 0373 dd_loss= 1.03118 gg_loss= 0.58360 cclass_loss= 1.12823 train_acc= 0.99167 test_acc= 0.72300 time= 2631.30599\n",
      "Epoch: 0374 dd_loss= 0.96902 gg_loss= 0.62809 cclass_loss= 1.13233 train_acc= 1.00000 test_acc= 0.72100 time= 2637.55980\n",
      "Epoch: 0375 dd_loss= 0.97792 gg_loss= 0.61735 cclass_loss= 1.12811 train_acc= 0.99167 test_acc= 0.71400 time= 2644.05429\n",
      "Epoch: 0376 dd_loss= 1.00969 gg_loss= 0.59850 cclass_loss= 1.12291 train_acc= 0.99167 test_acc= 0.71300 time= 2650.60214\n",
      "Epoch: 0377 dd_loss= 1.04701 gg_loss= 0.57463 cclass_loss= 1.12248 train_acc= 1.00000 test_acc= 0.72000 time= 2657.38103\n",
      "Epoch: 0378 dd_loss= 0.97058 gg_loss= 0.62763 cclass_loss= 1.12299 train_acc= 0.99167 test_acc= 0.71800 time= 2663.98893\n",
      "Epoch: 0379 dd_loss= 0.97959 gg_loss= 0.62447 cclass_loss= 1.12681 train_acc= 1.00000 test_acc= 0.71400 time= 2670.13127\n",
      "Epoch: 0380 dd_loss= 1.02215 gg_loss= 0.58402 cclass_loss= 1.12319 train_acc= 1.00000 test_acc= 0.71200 time= 2676.52218\n",
      "Epoch: 0381 dd_loss= 1.03349 gg_loss= 0.58320 cclass_loss= 1.12091 train_acc= 1.00000 test_acc= 0.71600 time= 2682.48398\n",
      "Epoch: 0382 dd_loss= 0.95888 gg_loss= 0.63663 cclass_loss= 1.12728 train_acc= 0.99167 test_acc= 0.71800 time= 2688.80100\n",
      "Epoch: 0383 dd_loss= 0.98401 gg_loss= 0.61928 cclass_loss= 1.13930 train_acc= 0.98333 test_acc= 0.71900 time= 2695.75098\n",
      "Epoch: 0384 dd_loss= 0.97132 gg_loss= 0.61792 cclass_loss= 1.13019 train_acc= 1.00000 test_acc= 0.71800 time= 2702.43092\n",
      "Epoch: 0385 dd_loss= 1.04792 gg_loss= 0.57255 cclass_loss= 1.13340 train_acc= 0.99167 test_acc= 0.71600 time= 2708.64731\n",
      "Epoch: 0386 dd_loss= 0.95143 gg_loss= 0.64024 cclass_loss= 1.13011 train_acc= 0.99167 test_acc= 0.71400 time= 2715.30313\n",
      "Epoch: 0387 dd_loss= 0.97122 gg_loss= 0.62332 cclass_loss= 1.13341 train_acc= 0.99167 test_acc= 0.71600 time= 2721.31837\n",
      "Epoch: 0388 dd_loss= 0.99098 gg_loss= 0.61295 cclass_loss= 1.13211 train_acc= 1.00000 test_acc= 0.71700 time= 2727.77278\n",
      "Epoch: 0389 dd_loss= 1.03304 gg_loss= 0.57800 cclass_loss= 1.11996 train_acc= 0.99167 test_acc= 0.71600 time= 2734.65734\n",
      "Epoch: 0390 dd_loss= 0.95092 gg_loss= 0.64351 cclass_loss= 1.13254 train_acc= 0.99167 test_acc= 0.71500 time= 2741.56923\n",
      "Epoch: 0391 dd_loss= 0.95764 gg_loss= 0.63172 cclass_loss= 1.12116 train_acc= 1.00000 test_acc= 0.71600 time= 2751.21590\n",
      "Epoch: 0392 dd_loss= 0.97154 gg_loss= 0.62342 cclass_loss= 1.13459 train_acc= 0.99167 test_acc= 0.71500 time= 2757.63166\n",
      "Epoch: 0393 dd_loss= 1.03632 gg_loss= 0.58116 cclass_loss= 1.12840 train_acc= 0.99167 test_acc= 0.71600 time= 2765.96513\n",
      "Epoch: 0394 dd_loss= 0.98773 gg_loss= 0.60981 cclass_loss= 1.12565 train_acc= 1.00000 test_acc= 0.71300 time= 2775.60800\n",
      "Epoch: 0395 dd_loss= 0.94498 gg_loss= 0.64101 cclass_loss= 1.12762 train_acc= 1.00000 test_acc= 0.71500 time= 2784.37694\n",
      "Epoch: 0396 dd_loss= 0.95808 gg_loss= 0.63013 cclass_loss= 1.12808 train_acc= 1.00000 test_acc= 0.71500 time= 2790.66826\n",
      "Epoch: 0397 dd_loss= 1.04265 gg_loss= 0.57238 cclass_loss= 1.12238 train_acc= 0.99167 test_acc= 0.71600 time= 2796.80694\n",
      "Epoch: 0398 dd_loss= 0.96452 gg_loss= 0.62441 cclass_loss= 1.12386 train_acc= 0.99167 test_acc= 0.71300 time= 2803.18305\n",
      "Epoch: 0399 dd_loss= 0.96725 gg_loss= 0.62882 cclass_loss= 1.12645 train_acc= 0.99167 test_acc= 0.70900 time= 2809.40460\n",
      "Epoch: 0400 dd_loss= 0.95641 gg_loss= 0.62980 cclass_loss= 1.13317 train_acc= 0.99167 test_acc= 0.71600 time= 2818.16298\n"
     ]
    }
   ],
   "source": [
    "# Set random seed\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "train_ratio = 120\n",
    "batch_size = 800\n",
    "\n",
    "namda = 0.4\n",
    "beta = 0.8\n",
    "print('batch_size:',batch_size)\n",
    "dataset = configs.FILES.citeseer\n",
    "lrate_gcn = configs.FILES.learning_rate\n",
    "print(\"train_ratio:\",train_ratio)\n",
    "\n",
    "x, _, adj_norm, labels, train_indexes, test_indexes = data_process.load_data(dataset, str(train_ratio), \n",
    "                                                                                x_flag='feature')\n",
    "\n",
    "node_num = adj_norm.shape[0]\n",
    "label_num = labels.shape[1]\n",
    "\n",
    "adj_norm_tuple = sparse.sparse_to_tuple(scipy.sparse.coo_matrix(adj_norm))\n",
    "feat_x_nn_tuple = sparse.sparse_to_tuple(scipy.sparse.coo_matrix(x))\n",
    "\n",
    "# node-node network train and validate masks\n",
    "nn_train_mask = np.zeros([node_num,])\n",
    "nn_test_mask = np.zeros([node_num,])\n",
    "\n",
    "# batch training indexes for gan\n",
    "gan_idx = tf.placeholder(tf.int32, shape=(batch_size,))\n",
    "real_sample_x = tf.placeholder(tf.float32, shape=[None, label_num])\n",
    "\n",
    "for i in train_indexes:\n",
    "    nn_train_mask[i] = 1\n",
    "#     nn_test_mask[i] = 0\n",
    "    \n",
    "for i in test_indexes:\n",
    "    nn_test_mask[i] = 1\n",
    "    \n",
    "# TensorFlow placeholders\n",
    "ph = {\n",
    "      'adj_norm': tf.sparse_placeholder(tf.float32, name=\"adj_norm\"),\n",
    "      'x': tf.sparse_placeholder(tf.float32, name=\"features\"),\n",
    "      'labels': tf.placeholder(tf.float32, name=\"node_labels\"),\n",
    "      'mask': tf.placeholder(tf.int32, shape=(node_num,))\n",
    "      }\n",
    "\n",
    "placeholders = {\n",
    "                'dropout_prob': tf.placeholder(tf.float32),\n",
    "                'num_features_nonzero': tf.placeholder(tf.int32)\n",
    "                }\n",
    "\n",
    "\n",
    "# the first layer\n",
    "edge_model = feat2struct.BuildGraphStruct(out_size = 30,\n",
    "                                        holders=placeholders,\n",
    "                                        name='e1',\n",
    "                                        act = tf.nn.relu,\n",
    "                                        feat_dropout = True)\n",
    "edge_w = edge_model(X = x.toarray())\n",
    "\n",
    "t_model = mg.GraphConvLayer(input_dim=x.shape[-1],\n",
    "                           output_dim=configs.FILES.h,\n",
    "                           name='nn_fc1',\n",
    "                           holders=placeholders,\n",
    "                           act=tf.nn.relu,\n",
    "                           dropout=True)\n",
    "  \n",
    "nn_fc1, embeds_c= t_model(adj_norm_c=[ph['adj_norm'],edge_w],\n",
    "                           x=ph['x'], x_c=ph['x'], sparse=True)\n",
    "\n",
    "# nn_fc1 = 0.8*nn_fc1 + 0.2*embeds_c\n",
    "\n",
    "# the second layer\n",
    "nn_dl, embeds_c2= mg.GraphConvLayer(input_dim=configs.FILES.h,\n",
    "                           output_dim=label_num,\n",
    "                           name='nn_dl',\n",
    "                           holders=placeholders,\n",
    "                           act=tf.nn.softmax,\n",
    "                           dropout=True)(adj_norm_c=[ph['adj_norm'],edge_w],\n",
    "                                           x=nn_fc1, x_c=embeds_c)\n",
    "                           \n",
    "# nn_dl = 0.8*nn_dl + 0.2*embeds_c2\n",
    "\n",
    "# the discriminative network\n",
    "real_x = tf.gather(embeds_c2, gan_idx)\n",
    "fake_x = tf.gather(nn_dl, gan_idx)\n",
    "\n",
    "gan_model = ma.Discriminator(x_dim=label_num, h_dim=configs.FILES.hidden_dim)\n",
    "D_real, D_logit_real = gan_model(real_sample_x)\n",
    "D_fake, D_logit_fake = gan_model(fake_x)\n",
    "\n",
    "\n",
    "def frobenius_distance(embeds_c2, original_x, edge_w, original_adj):\n",
    "    original_x=tf.nn.softmax(original_x)\n",
    "    embeds_c2=tf.nn.softmax(embeds_c2)\n",
    "    loss =tf.norm(tf.matmul(original_x,tf.transpose(original_x))-tf.matmul(embeds_c2,tf.transpose(embeds_c2)),\n",
    "                       ord='fro', axis=(0,1))\n",
    "    return loss\n",
    "\n",
    "def gan_sigmoid_cross_entropy(D_real_preds, D_fake_preds):\n",
    "    \n",
    "    D_loss_real = tf.nn.sigmoid_cross_entropy_with_logits(logits=D_real_preds, \n",
    "                                                          labels=tf.ones_like(D_real_preds))\n",
    "    D_loss_fake = tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake_preds, \n",
    "                                                          labels=tf.zeros_like(D_fake_preds))\n",
    "    loss_g = tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake_preds, \n",
    "                                                     labels=tf.ones_like(D_fake_preds))\n",
    "    \n",
    "    loss_d = tf.reduce_mean(D_loss_real) + tf.reduce_mean(D_loss_fake)\n",
    "    loss_g = tf.reduce_mean(loss_g)\n",
    "#     return tf.reduce_mean(D_loss_real) + tf.reduce_mean(D_loss_fake) + tf.reduce_mean(loss_g)\n",
    "    return loss_d, loss_g\n",
    "\n",
    "def masked_sigmoid_softmax_cross_entropy(preds, labels, mask):\n",
    "    \"\"\"Sigmoid softmax cross-entropy loss with masking.\"\"\"\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=preds, labels=labels)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    mask /= tf.reduce_mean(mask)\n",
    "    loss *= mask\n",
    "#     for var in tf.trainable_variables():\n",
    "    for var in t_model.var.values():\n",
    "        var = tf.cast(var, dtype=tf.float32)\n",
    "        loss += configs.FILES.weight_decay * tf.nn.l2_loss(var)\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "def masked_accuracy(preds, labels, mask):\n",
    "    \"\"\"Accuracy with masking.\"\"\"\n",
    "    correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(labels, 1))\n",
    "    accuracy_all = tf.cast(correct_prediction, tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    mask /= tf.reduce_mean(mask)\n",
    "    accuracy_all *= mask\n",
    "    \n",
    "    return tf.reduce_mean(accuracy_all), correct_prediction\n",
    "\n",
    "# calculate the classification accuracy per classes   \n",
    "def precision_per_class(preds, labels, mask):\n",
    "    import heapq\n",
    "    mask = mask.astype(int)\n",
    "    labels = labels.astype(int)\n",
    "    val_indexes = np.where(mask==1)[0]\n",
    "    pred_true_labels = {}\n",
    "    \n",
    "    y_true = []\n",
    "    y_pred = []    \n",
    "    \n",
    "    for i in val_indexes:\n",
    "        pred_probs_i = preds[i]\n",
    "        true_raw_i = labels[i]\n",
    "        \n",
    "        pred_label_i = heapq.nlargest(np.sum(true_raw_i),range(len(pred_probs_i)), \n",
    "                                      pred_probs_i.take)\n",
    "        true_label_i = np.where(true_raw_i==1)[0]\n",
    "        pred_true_labels[i] = (pred_label_i, true_label_i)\n",
    "        \n",
    "        y_true.append(true_label_i)\n",
    "        y_pred.append(pred_label_i)\n",
    "        \n",
    "    accuracy_per_classes = metrics.evaluate(pred_true_labels)\n",
    "    \n",
    "    from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
    "    \n",
    "    mat = confusion_matrix(y_true, y_pred)\n",
    "    print(mat)        \n",
    "        \n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    test_y = labels[val_indexes]\n",
    "    test_pred = preds[val_indexes]\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(test_y.ravel(), test_pred.ravel())\n",
    "    auc = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "    \n",
    "    return accuracy_per_classes, auc\n",
    "    \n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    test_y = labels[val_indexes]\n",
    "    test_pred = preds[val_indexes]\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(test_y.ravel(), test_pred.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "    print('micro_auc=',roc_auc[\"micro\"])\n",
    "    \n",
    "    return accuracy_per_classes\n",
    "\n",
    "with tf.name_scope('optimizer'):\n",
    "    # graph construct loss L_{cont}\n",
    "    graph_build_loss = frobenius_distance(embeds_c2,\n",
    "                                          tf.dtypes.cast(x.toarray(), tf.float32),\n",
    "                                          tf.sparse.to_dense(edge_w),\n",
    "                                          tf.dtypes.cast(adj_norm.toarray(), tf.float32))\n",
    "    \n",
    "    # discriminator training loss L_{gan}\n",
    "    d_loss, g_loss = gan_sigmoid_cross_entropy(D_real_preds=D_logit_real, \n",
    "                                               D_fake_preds=D_logit_fake)\n",
    "    \n",
    "    # semi-supervised node classification loss L_{gcn}\n",
    "    class_loss = masked_sigmoid_softmax_cross_entropy(preds=nn_dl, \n",
    "                                                labels=ph['labels'], \n",
    "                                                mask=ph['mask'])\n",
    "#     \n",
    "    loss1 = namda*graph_build_loss + 0.8*d_loss + class_loss\n",
    "    loss2 = namda*graph_build_loss + 0.8*g_loss + class_loss\n",
    "        \n",
    "    accuracy, correct_prediction = masked_accuracy(preds=nn_dl, \n",
    "                               labels=ph['labels'], mask=ph['mask'])\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=lrate_gcn)\n",
    "#     opt_op = optimizer.minimize(loss)    \n",
    "    opt_op1 = optimizer.minimize(loss1)    \n",
    "    opt_op2 = optimizer.minimize(loss2)    \n",
    "\n",
    "feed_dict_train = {ph['adj_norm']: adj_norm_tuple,\n",
    "                      ph['x']: feat_x_nn_tuple,\n",
    "#                       ph['labels']: labels,\n",
    "                      ph['labels']: labels.toarray(),\n",
    "                      ph['mask']: nn_train_mask,\n",
    "                      placeholders['dropout_prob']: configs.FILES.dropout_prob,\n",
    "                      placeholders['num_features_nonzero']: feat_x_nn_tuple[1].shape,\n",
    "                      gan_idx: None,\n",
    "                      real_sample_x: None\n",
    "                      }\n",
    "feed_dict_val = {ph['adj_norm']: adj_norm_tuple,\n",
    "                    ph['x']: feat_x_nn_tuple,\n",
    "#                     ph['labels']: labels,\n",
    "                    ph['labels']: labels.toarray(),\n",
    "                    ph['mask']: nn_test_mask,\n",
    "                    placeholders['dropout_prob']: 0.,\n",
    "                    placeholders['num_features_nonzero']: feat_x_nn_tuple[1].shape,\n",
    "                    gan_idx: None\n",
    "                    }\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "epochs = 400\n",
    "save_every = 1    \n",
    "    \n",
    "t = time.time()\n",
    "# Train model\n",
    "times = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Training node embedding\n",
    "#     _, train_loss = sess.run(\n",
    "#         (opt_op, loss), feed_dict=feed_dict_train)\n",
    "    # obtain the training sample indexes for gan\n",
    "    begin = epoch * batch_size\n",
    "    end = epoch * batch_size + batch_size\n",
    "    batch_idx = []\n",
    "    for i in range(begin, end):\n",
    "        idx = i % node_num\n",
    "        batch_idx.append(idx)\n",
    "\n",
    "    if epoch % save_every == 0:\n",
    "        feed_dict_val.update(({gan_idx: batch_idx}))\n",
    "        val_acc, test_nn_dl, real_x_v = sess.run((accuracy, nn_dl, real_x), feed_dict=feed_dict_val)\n",
    "    feed_dict_train.update(({gan_idx: batch_idx, real_sample_x: real_x_v}))\n",
    "    sess.run((opt_op1, graph_build_loss, accuracy, nn_dl),feed_dict=feed_dict_train) \n",
    "    _, dd_loss, gg_loss, cclass_loss, train_acc, train_nn_dl = sess.run((opt_op2, d_loss, g_loss, class_loss, accuracy, nn_dl), \n",
    "                                                              feed_dict=feed_dict_train) \n",
    "    \n",
    "    print(\"Epoch:\", '%04d' % (epoch + 1),\n",
    "          \"dd_loss=\", \"{:.5f}\".format(dd_loss),\n",
    "          \"gg_loss=\", \"{:.5f}\".format(gg_loss),\n",
    "          \"cclass_loss=\", \"{:.5f}\".format(cclass_loss),\n",
    "          \"train_acc=\", \"{:.5f}\".format(train_acc),\n",
    "#               \"train_auc\", \"{:.5f}\".format(train_auc),\n",
    "          \"test_acc=\", \"{:.5f}\".format(val_acc),\n",
    "#               \"test_auc\", \"{:.5f}\".format(test_auc),\n",
    "          \"time=\", \"{:.5f}\".format(time.time() - t))\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        times.append(time.time() - t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
